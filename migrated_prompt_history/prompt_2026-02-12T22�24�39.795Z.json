[
  {
    "id": 0,
    "author": "user",
    "payload": {
      "type": "text",
      "text": "Sometimes on users websites a page can be linking to other page as /page/seo+new, and other page as /page/seo-new, /page/Seo+NEw etc,, but the page real link is canconial is /page/seo-new, but other pages are calling him using errors based internal links,,, the system should need to must have specialist intellligent system page for this issue, and similar ones, to find pin point location of which page  is linking using other than canonical of the page.Add new feature for this which checks if pages are calling a page using different slugs or not, pin point location also which page is calling /best-groups slug as /Best+groups or something which creates duplicate links redirects loop",
      "files": [],
      "fileUploads": [],
      "cssBlocks": [],
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770900748,
      "nanos": 155000000
    }
  },
  {
    "id": 1,
    "author": "model",
    "payload": {
      "type": "thinking",
      "text": "**Considering Internal Links**\n\nI'm now focusing on a new feature for the SEO auditing tool, specifically detecting inconsistencies in internal links. My current direction involves identifying instances where internal links point to a page with a URL different from the canonical URL. This requires an understanding of how to compare the URLs to detect the inconsistencies.\n\n\n**Defining True Identity**\n\nI'm now zeroing in on defining a page's \"True Identity\" within the crawler. I'm focusing on handling inconsistencies introduced by differences in capitalization, encoding, and characters that might be valid, but not the canonical version, of a URL. This involves identifying all unique pages and resolving them to their canonical or crawled URL. I plan to build a system to analyze incoming internal links, flag incorrect versions, and generate a specific report. I'm thinking about updating the `types.ts` file to include detailed incoming link information to properly capture these issues.\n\n\n**Tracking Link Sources**\n\nI realize the `internalLinks` structure needs an overhaul. I must track the *origin* of each link, not just the target. My plan involves inverting the graph and tracking source URLs and link details within `postAnalysis.ts`. The crawler extracts links, but the post-analysis phase offers the best place to resolve and compare canonical URLs against linked URLs. This involves building a map, iterating pages, and resolving the target URL.\n\n\n**Deciding Issue Location**\n\nI've been wrestling with where to surface these internal link inconsistencies. While the target page angle has merit, the source page feels more actionable for the user. I'm leaning toward an issue on the source page, labeled \"Broken/Non-Canonical Internal Link,\" with a clear description pointing out the incorrect target. I might also flag the target page with \"Inconsistent Inlinks Detected\" to highlight the issue. This strategy aligns with the user's need to pinpoint the linking page.\n\n\n**Mapping Canonical URLs**\n\nI am now focusing on the `postAnalysis.ts` file, and constructing a `canonicalMap<string, string>`, with the page URL as the key, and the canonical URL as the value, or the original if no canonical exists. I have to account for cases where \"bad\" links may not be crawled. The logic needs to handle normalized URLs. I am considering cases like `/Best+groups` and `/best-groups`, which might lead to duplicate content or 301 redirects, and I am building the process to match all this information.\n\n\n**Analyzing Link Target Identity**\n\nI'm now diving into the `targetPage` details and the data in `PageData`. If a target page exists, I will examine its canonical URL and compare it to the linked URL. If there's a discrepancy, I'll flag the source page as \"Links to Non-Canonical.\" I am also focusing on scenarios with potentially non-existent URLs or redirects.\n\n\n**Addressing Uncrawled Targets**\n\nI'm now thinking about scenarios where internal links point to URLs that weren't crawled, perhaps due to maximum crawl depth. Even in these cases, I'm considering a strategy of strict string consistency if I have a \"known URLs\" list. I have a revised logic plan. I'll build a \"Canonical Registry\" from crawled pages with a 200 status, mapping the Canonical URL to a set of crawled URLs claiming that canonical. Alternatively, I may simply map the crawled URL to the preferred version (canonical if available, else self). Then, when I iterate through the pages, for each link, I can use this data to determine if an issue should be created.\n\n\n**Designing Link Intelligence**\n\nI'm now focused on the UI. The user's request for a dedicated \"Link Intelligence\" feature has shaped my approach. I'm planning a new Tab or Section in the dashboard, possibly a \"Link Consistency\" button. This view will feature a \"Bad Links\" table, displaying Source URL, Link Anchor, and other link attributes. I am realizing the need to parse anchor text from `<a>` elements in `crawler.ts` and updating `types.ts` to accommodate the change.\n\n\n**Implementing Link Consistency**\n\nI am now focused on a step-by-step implementation plan. I'm starting with modifications to `postAnalysis.ts` for the new logic, then adding the \"Link Consistency\" tab to `App.tsx` and potentially a metric to `Dashboard.tsx`. I am also working on designing the \"Link Consistency\" view to display source pages, \"bad\" URLs, target canonical URLs, and anchor text. I have also identified a \"Link to Redirect\" issue and my proposed logic handles this edge case.\n\n\n**Refining Link Structure**\n\nI'm now revising my approach to internal link data structures. I've decided to update `types.ts` to include an interface called `LinkData`, which contains the `url` and `text` properties to capture link anchor text. I plan to rename `internalLinks` to `links` and modify `PageData` to reflect this change, making `internalLinks` an array of `LinkData` objects. I am now updating the link extraction logic to get the `innerText` of `<a>` tags and to clean up whitespace.\n\n\n**Generating New Issues**\n\nI'm now implementing the issue generation logic and defining the specifics of how to report \"link consistency\" issues. I'll be focused on `postAnalysis.ts` for the \"link consistency\" issue, and on `App.tsx` to add a new view for the link audit. The new issue type will be `link-consistency` with a `MEDIUM` severity. I will ensure this new issue format displays nicely, including the message and description fields. I'm also planning to modify `types.ts` and `crawler.ts` to reflect the updated internal link data, as well as updating the auditor if necessary.\n\n\n**Implementing Cross-References**\n\nI'm now focusing on `postAnalysis.ts`. I'll iterate through `internalLinks` to check the URL against `urlMap`. If the target exists, I will verify the target's canonical URL and status to generate appropriate issues. I'll need to update `IssueDetails.tsx` to display link consistency information. Also, I will be adding the \"Link Consistency\" tab to `App.tsx` and filtering for the 'link-consistency' issue type.\n\n\n**Defining Issue Message**\n\nI've finalized the `Issue` type, specifically `Link Consistency`. The ID will be `link-inconsistent-slug`, with the message \"Inconsistent Link Target\" and description: \"Link with anchor '${text}' points to '${url}' but the target page canonical is '${canonical}'.\" I have updated the `types.ts` file to reflect the updated `internalLinks` type, and determined that the `auditor.ts` file only needs an import adjustment and no logic change. However, I have identified that the logic in the `postAnalysis.ts` file requires updating.\n\n\n**Refining Internal Logic**\n\nI am now updating `postAnalysis.ts` to implement link consistency checks and handling redirect scenarios. I'm focusing on iterating through the updated `internalLinks` array to retrieve the target URL and link details. The strategy involves checking for redirects first, before generating an issue. I'll then move on to generating issues for links to non-canonical URLs, with a precise message describing the link, the target, and canonical URL.\n\n\n**Addressing Uncrawled Targets**\n\nI'm now grappling with how to handle links to uncrawled pages. If the target page isn't in `urlMap`, due to a crawl limit, I will create a new issue for this, but only if \"strictness\" of validation is appropriate. This covers the edge case of relative URLs being resolved to absolute paths. My current strategy is to prioritize what can be validated, and flag the issue accordingly.\n\n\n**Designing Link Inspector**\n\nI'm now integrating the \"Link Inspector\" into the application's UI. I'll add a sidebar item, likely \"Link Inspector,\" that activates a table-focused view. The plan is to create a `LinkAudit` component or integrate directly into the existing dashboard and issue list, possibly via the sidebar with \"Link Consistency\" filtering. I'll also add details to `IssueDetails` for `link-inconsistent` issues, and I'll adjust the data structure by renaming `internalLinks` to `links: {url:string, text:string}[]`. I'll also modify the files `types.ts`, `crawler.ts`, `postAnalysis.ts`, and `auditor.ts` to reflect the updated `internalLinks` property.\n\n\n**Refining Issue Presentation**\n\nI'm now refining the presentation of link consistency issues. I'll include anchor text in the issue description, making it easier to identify the problematic link. The description will be formatted to include anchor text, the linked URL, and the canonical URL, improving readability and user clarity. Also, I've confirmed that the crawler handles fetch loops, eliminating this concern.\n\n\n**Implementing Link Auditing**\n\nI'm now integrating logic for \"Link to Redirect\" scenarios. If a link points to a URL that redirects, I'll generate a \"Link to Redirect\" issue. This involves checking for redirects in `postAnalysis.ts`. I am also working on `App.tsx` and adding a new sidebar button for \"Link Consistency\". I've finalized updates to `types.ts` and `crawler.ts` to capture link text. Finally, `postAnalysis.ts` needs further refinement.\n\n\n**Integrating Link Inspection**\n\nI am now focusing on integrating the \"Link Inspector\" view into `App.tsx` and refining the issue details in `IssueDetails.tsx`. I will also add a \"Link Consistency\" sidebar item. I plan to add \"Bad Links\" data to `IssueDetails` if the active tab is `LINKS`. I am also reviewing `auditor.ts` and confirmed the changes I will make in `types.ts` will not affect it, as the file doesn't iterate through `internalLinks`.\n\n\n",
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770900748,
      "nanos": 155000000
    },
    "doneTimestamp": {
      "seconds": 1770900847,
      "nanos": 726000000
    }
  },
  {
    "id": 2,
    "author": "model",
    "payload": {
      "type": "generationTable",
      "entries": [
        {
          "path": "types.ts",
          "generationType": 1,
          "isDone": true,
          "description": "Update PageData to include anchor text in internal links for deeper link analysis.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nexport enum IssueSeverity {\n  CRITICAL = 'CRITICAL',\n  HIGH = 'HIGH',\n  MEDIUM = 'MEDIUM',\n  LOW = 'LOW',\n  INFO = 'INFO',\n}\n\nexport interface Issue {\n  id: string;\n  type: string;\n  severity: IssueSeverity;\n  message: string;\n  description: string;\n  recommendation: string;\n}\n\nexport interface ImageAsset {\n  src: string;\n  alt: string;\n  title: string | null;\n  width: string | null;\n  height: string | null;\n  loading: string | null;\n  isExternal: boolean;\n}\n\nexport interface SchemaData {\n  type: string;\n  raw: string; // JSON string\n  isValid: boolean;\n  error?: string;\n}\n\nexport interface InternalLink {\n  url: string;\n  text: string;\n}\n\nexport interface PageData {\n  url: string;\n  status: number;\n  loadTime: number; // ms\n  size: number; // bytes (approx)\n  \n  // Content Metrics\n  wordCount: number;\n  textRatio: number; // 0 to 100\n  contentHash: string; // For duplicate detection\n  domNodeCount: number; // DOM Complexity\n  \n  // Meta\n  title: string | null;\n  description: string | null;\n  h1: string | null;\n  h2s: string[];\n  h3s: string[];\n  canonical: string | null;\n  metaRobots: string | null;\n  viewport: string | null;\n  charset: string | null;\n  \n  // International & Pagination\n  hreflangs: { lang: string; url: string }[];\n  relNext: string | null;\n  relPrev: string | null;\n  \n  // Social\n  ogTitle: string | null;\n  ogImage: string | null;\n  twitterCard: string | null;\n  \n  // Technical\n  analyticsId: string | null; \n  deprecatedTags: string[];\n  emailsFound: string[];\n  schemas: SchemaData[];\n  \n  // Assets\n  images: ImageAsset[];\n  scriptCount: number;\n  cssCount: number;\n  inlineCssCount: number;\n  \n  // Links\n  internalLinks: InternalLink[];\n  externalLinks: string[];\n  unsafeAnchorCount: number;\n  \n  // Calculated Metrics (Post-Crawl)\n  inRank: number; // Internal PageRank (0-10)\n  inlinksCount: number; \n  \n  // Audit Results\n  issues: Issue[];\n  \n  // Flags\n  isIndexable: boolean;\n  depth: number;\n}\n\nexport interface CrawlStats {\n  pagesCrawled: number;\n  queueLength: number;\n  currentDepth: number;\n  startTime: number;\n  endTime: number | null;\n  status: 'IDLE' | 'CRAWLING' | 'ANALYZING' | 'PAUSED' | 'COMPLETED' | 'ERROR';\n}\n\nexport interface CrawlSettings {\n  maxDepth: number;\n  maxPages: number;\n  crawlSpeed: number; // delay in ms\n  ignoreRobotsTxt: boolean;\n  proxyUrl: string; // CORS proxy\n  userAgent: string;\n}\n\nexport const DEFAULT_SETTINGS: CrawlSettings = {\n  maxDepth: 3,\n  maxPages: 50,\n  crawlSpeed: 600, \n  ignoreRobotsTxt: true,\n  proxyUrl: 'https://corsproxy.io/?',\n  userAgent: 'AURORA-X-Bot/3.0',\n};"
            }
          ]
        },
        {
          "path": "services/crawler.ts",
          "generationType": 1,
          "isDone": true,
          "description": "Extract anchor text for internal links to support link consistency auditing.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nimport { PageData, CrawlSettings, ImageAsset, SchemaData, InternalLink } from '../types';\nimport { analyzePage } from './auditor';\n\n// Helper to normalize URLs\nconst normalizeUrl = (url: string): string => {\n  let u = url.trim();\n  // Auto-prepend protocol if missing\n  if (!u.match(/^https?:\\/\\//i)) {\n    u = 'https://' + u;\n  }\n  try {\n    const urlObj = new URL(u);\n    urlObj.hash = ''; // Remove fragments\n    return urlObj.toString();\n  } catch (e) {\n    return '';\n  }\n};\n\n// Simple DJB2 hash for content fingerprinting\nconst simpleHash = (str: string): string => {\n  let hash = 5381;\n  for (let i = 0; i < str.length; i++) {\n    hash = (hash * 33) ^ str.charCodeAt(i);\n  }\n  return (hash >>> 0).toString(16);\n};\n\ninterface QueueItem {\n  url: string;\n  depth: number;\n}\n\nexport class Crawler {\n  private visited = new Set<string>();\n  private queue: QueueItem[] = [];\n  private settings: CrawlSettings;\n  private isRunning = false;\n  private onPageCrawled: (page: PageData) => void;\n  private onComplete: () => void;\n\n  constructor(\n    settings: CrawlSettings, \n    onPageCrawled: (page: PageData) => void,\n    onComplete: () => void\n  ) {\n    this.settings = settings;\n    this.onPageCrawled = onPageCrawled;\n    this.onComplete = onComplete;\n  }\n\n  start(startUrl: string) {\n    if (this.isRunning) return;\n    this.isRunning = true;\n    this.visited.clear();\n    const normalized = normalizeUrl(startUrl);\n    if (!normalized) {\n        alert(\"Invalid Start URL\");\n        this.stop();\n        return;\n    }\n    this.queue = [{ url: normalized, depth: 0 }];\n    this.processQueue();\n  }\n\n  stop() {\n    this.isRunning = false;\n    this.onComplete();\n  }\n\n  private async processQueue() {\n    if (!this.isRunning) return;\n    if (this.queue.length === 0 || this.visited.size >= this.settings.maxPages) {\n      this.stop();\n      return;\n    }\n\n    const item = this.queue.shift();\n    if (!item) return;\n\n    if (this.visited.has(item.url)) {\n      this.processQueue();\n      return;\n    }\n\n    if (item.depth > this.settings.maxDepth) {\n        this.processQueue(); // Skip but keep processing\n        return;\n    }\n\n    this.visited.add(item.url);\n    await this.crawlPage(item);\n\n    // Randomize delay slightly to appear more human\n    const delay = this.settings.crawlSpeed + (Math.random() * 200);\n    setTimeout(() => {\n      this.processQueue();\n    }, delay);\n  }\n\n  private async crawlPage(item: QueueItem) {\n    const startTime = performance.now();\n    \n    // Define Proxy Fallback Strategy\n    const proxies = [\n        this.settings.proxyUrl,                  // User configured (Default: corsproxy.io)\n        'https://api.allorigins.win/raw?url=',   // Fallback 1\n        'https://thingproxy.freeboard.io/fetch/' // Fallback 2\n    ].filter(p => !!p);\n    \n    // Deduplicate proxies\n    const uniqueProxies = [...new Set(proxies)];\n\n    let rawHtml = '';\n    let status = 0;\n    let fetchSuccess = false;\n\n    // Try proxies in order\n    for (const proxyBase of uniqueProxies) {\n        if (fetchSuccess) break;\n\n        // Construct Proxy URL (Always encodeURIComponent for safety with these proxies)\n        // corsproxy.io supports ?url= or just ? but encoding is safer for params\n        const proxyUrl = `${proxyBase}${encodeURIComponent(item.url)}`;\n        \n        const MAX_RETRIES = 2; // Retries per proxy\n        \n        for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {\n            try {\n                const controller = new AbortController();\n                const timeoutId = setTimeout(() => controller.abort(), 15000); // 15s timeout\n\n                const response = await fetch(proxyUrl, {\n                    signal: controller.signal,\n                    headers: {\n                        'Cache-Control': 'no-cache',\n                        // Some proxies require X-Requested-With to allow access\n                        'X-Requested-With': 'XMLHttpRequest' \n                    }\n                });\n                clearTimeout(timeoutId);\n\n                status = response.status;\n                \n                // If we get a 200-500 response, the network request worked.\n                // If we get a 0 or throw, it's a network error.\n                \n                // Note: Some proxies return 200 even if they fail internally, returning JSON error.\n                // We will check that later in parsing.\n                \n                if (status === 429) {\n                    throw new Error('Rate Limited');\n                }\n\n                rawHtml = await response.text();\n                \n                // Check if the proxy returned an error message in HTML disguise\n                if (rawHtml.includes('Could not fetch URL') || rawHtml.includes('Access to the requested resource is forbidden')) {\n                    throw new Error('Proxy failed to fetch target');\n                }\n\n                fetchSuccess = true;\n                break; // Break retry loop\n            } catch (error: any) {\n                console.warn(`Proxy ${proxyBase} failed for ${item.url} (Attempt ${attempt}): ${error.message}`);\n                \n                if (attempt < MAX_RETRIES) {\n                     await new Promise(r => setTimeout(r, 1000));\n                }\n            }\n        }\n    }\n\n    if (!fetchSuccess) {\n        console.error(`All proxies failed for ${item.url}`);\n        // Log a synthetic \"Network Error\" page so the user knows it failed\n        const failData: PageData = this.createFailedPageData(item.url, 0);\n        this.onPageCrawled(failData);\n        return;\n    }\n\n    const loadTime = Math.round(performance.now() - startTime);\n    const size = new Blob([rawHtml]).size;\n\n    const parser = new DOMParser();\n    const doc = parser.parseFromString(rawHtml, 'text/html');\n\n    // --- Deep Extraction ---\n\n    // 1. Content & Structure\n    const bodyText = doc.body ? doc.body.innerText : '';\n    const cleanText = bodyText.replace(/\\s+/g, ' ').trim();\n    const wordCount = cleanText.length > 0 ? cleanText.split(' ').length : 0;\n    const textRatio = size > 0 ? Math.round((cleanText.length / rawHtml.length) * 100) : 0;\n    const contentHash = simpleHash(cleanText);\n    const domNodeCount = doc.getElementsByTagName('*').length;\n\n    // 2. Meta Headers\n    const title = doc.querySelector('title')?.innerText.trim() || null;\n    const description = doc.querySelector('meta[name=\"description\"]')?.getAttribute('content')?.trim() || null;\n    const viewport = doc.querySelector('meta[name=\"viewport\"]')?.getAttribute('content') || null;\n    const charset = doc.characterSet || doc.inputEncoding;\n\n    // 3. Header Structure\n    const h1 = doc.querySelector('h1')?.innerText.trim() || null;\n    const h2s = Array.from(doc.querySelectorAll('h2')).map(el => el.innerText.trim()).filter(Boolean);\n    const h3s = Array.from(doc.querySelectorAll('h3')).map(el => el.innerText.trim()).filter(Boolean);\n\n    // 4. Canonicals, Robots, Pagination, Hreflang\n    const canonicalLink = doc.querySelector('link[rel=\"canonical\"]')?.getAttribute('href') || null;\n    const metaRobots = doc.querySelector('meta[name=\"robots\"]')?.getAttribute('content') || null;\n    const relNext = doc.querySelector('link[rel=\"next\"]')?.getAttribute('href') || null;\n    const relPrev = doc.querySelector('link[rel=\"prev\"]')?.getAttribute('href') || null;\n\n    const hreflangs = Array.from(doc.querySelectorAll('link[rel=\"alternate\"][hreflang]')).map(el => ({\n        lang: el.getAttribute('hreflang') || '',\n        url: el.getAttribute('href') || ''\n    }));\n\n    // Resolve Canonical\n    let absoluteCanonical = null;\n    if (canonicalLink) {\n      try {\n        absoluteCanonical = new URL(canonicalLink, item.url).toString();\n      } catch (e) {\n        absoluteCanonical = canonicalLink; \n      }\n    }\n\n    // 5. Social Tags\n    const ogTitle = doc.querySelector('meta[property=\"og:title\"]')?.getAttribute('content') || null;\n    const ogImage = doc.querySelector('meta[property=\"og:image\"]')?.getAttribute('content') || null;\n    const twitterCard = doc.querySelector('meta[name=\"twitter:card\"]')?.getAttribute('content') || null;\n\n    // 6. Schema.org Extraction\n    const schemas: SchemaData[] = [];\n    const scriptTags = Array.from(doc.querySelectorAll('script[type=\"application/ld+json\"]'));\n    scriptTags.forEach(script => {\n        const content = script.textContent || '';\n        try {\n            const json = JSON.parse(content);\n            schemas.push({\n                type: json['@type'] || 'Unknown',\n                raw: JSON.stringify(json, null, 2),\n                isValid: true\n            });\n        } catch (e) {\n            schemas.push({\n                type: 'Invalid JSON',\n                raw: content,\n                isValid: false,\n                error: (e as Error).message\n            });\n        }\n    });\n\n    // 7. Assets & Images (Deep)\n    const images: ImageAsset[] = Array.from(doc.querySelectorAll('img')).map(img => {\n        const src = img.getAttribute('src') || '';\n        let isExternal = false;\n        try { isExternal = new URL(src, item.url).hostname !== new URL(item.url).hostname; } catch(e){}\n        \n        return {\n            src,\n            alt: img.getAttribute('alt') || '',\n            title: img.getAttribute('title') || null,\n            width: img.getAttribute('width') || null,\n            height: img.getAttribute('height') || null,\n            loading: img.getAttribute('loading') || null,\n            isExternal\n        };\n    });\n    \n    const scripts = Array.from(doc.querySelectorAll('script'));\n    const cssLinks = Array.from(doc.querySelectorAll('link[rel=\"stylesheet\"]'));\n    const inlineCssCount = Array.from(doc.querySelectorAll('[style]')).length;\n\n    // 8. Security & Tech\n    let analyticsId = null;\n    if (rawHtml.match(/UA-\\d+-\\d+/)) analyticsId = rawHtml.match(/UA-\\d+-\\d+/)?.[0] || null;\n    else if (rawHtml.match(/G-[A-Z0-9]+/)) analyticsId = rawHtml.match(/G-[A-Z0-9]+/)?.[0] || null;\n\n    const deprecatedTags = [];\n    if (doc.querySelector('center')) deprecatedTags.push('<center>');\n    if (doc.querySelector('font')) deprecatedTags.push('<font>');\n    if (doc.querySelector('marquee')) deprecatedTags.push('<marquee>');\n\n    const emailsFound = (bodyText.match(/([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9._-]+)/gi) || [])\n        .filter((v, i, a) => a.indexOf(v) === i); \n\n    const unsafeAnchorCount = Array.from(doc.querySelectorAll('a[target=\"_blank\"]:not([rel*=\"noopener\"])')).length;\n\n    // 9. Link Extraction\n    const internalLinks: InternalLink[] = [];\n    const externalLinks: string[] = [];\n    \n    let baseDomain = '';\n    try {\n      baseDomain = new URL(item.url).hostname;\n    } catch(e) {}\n\n    const linkElements = Array.from(doc.querySelectorAll('a[href]'));\n    linkElements.forEach(el => {\n      const href = el.getAttribute('href');\n      if (!href) return;\n      if (href.startsWith('javascript:') || href.startsWith('mailto:') || href.startsWith('tel:')) return;\n\n      const anchorText = (el as HTMLElement).innerText?.replace(/\\s+/g, ' ').trim().slice(0, 50) || 'Empty/Image';\n\n      try {\n        const absoluteUrl = new URL(href, item.url).toString();\n        const urlObj = new URL(absoluteUrl);\n        \n        // Remove hash from crawled links\n        urlObj.hash = '';\n        const cleanUrl = urlObj.toString();\n        \n        if (urlObj.hostname === baseDomain) {\n            internalLinks.push({ url: cleanUrl, text: anchorText });\n            if (!this.visited.has(cleanUrl)) {\n                 this.queue.push({ url: cleanUrl, depth: item.depth + 1 });\n            }\n        } else {\n            externalLinks.push(cleanUrl);\n        }\n      } catch (e) {\n        // Invalid URL\n      }\n    });\n\n    const pageData: PageData = {\n      url: item.url,\n      status,\n      loadTime,\n      size,\n      wordCount,\n      textRatio,\n      contentHash,\n      domNodeCount,\n      title,\n      description,\n      h1,\n      h2s,\n      h3s,\n      canonical: absoluteCanonical,\n      metaRobots,\n      viewport,\n      charset: charset || 'unknown',\n      relNext,\n      relPrev,\n      hreflangs,\n      ogTitle,\n      ogImage,\n      twitterCard,\n      analyticsId,\n      deprecatedTags,\n      emailsFound,\n      schemas,\n      images,\n      scriptCount: scripts.length,\n      cssCount: cssLinks.length,\n      inlineCssCount,\n      internalLinks,\n      externalLinks,\n      unsafeAnchorCount,\n      inRank: 0, \n      inlinksCount: 0, \n      issues: [],\n      isIndexable: status === 200 && !metaRobots?.includes('noindex'),\n      depth: item.depth,\n    };\n\n    pageData.issues = analyzePage(pageData);\n\n    this.onPageCrawled(pageData);\n  }\n\n  // Helper to create a page data object for failed requests\n  private createFailedPageData(url: string, status: number): PageData {\n      const p: PageData = {\n          url,\n          status,\n          loadTime: 0,\n          size: 0,\n          wordCount: 0,\n          textRatio: 0,\n          contentHash: '',\n          domNodeCount: 0,\n          title: null,\n          description: null,\n          h1: null,\n          h2s: [],\n          h3s: [],\n          canonical: null,\n          metaRobots: null,\n          viewport: null,\n          charset: null,\n          relNext: null,\n          relPrev: null,\n          hreflangs: [],\n          ogTitle: null,\n          ogImage: null,\n          twitterCard: null,\n          analyticsId: null,\n          deprecatedTags: [],\n          emailsFound: [],\n          schemas: [],\n          images: [],\n          scriptCount: 0,\n          cssCount: 0,\n          inlineCssCount: 0,\n          internalLinks: [],\n          externalLinks: [],\n          unsafeAnchorCount: 0,\n          inRank: 0,\n          inlinksCount: 0,\n          issues: [],\n          isIndexable: false,\n          depth: 0\n      };\n      p.issues = analyzePage(p);\n      return p;\n  }\n}\n"
            }
          ]
        },
        {
          "path": "services/postAnalysis.ts",
          "generationType": 1,
          "isDone": true,
          "description": "Implement deep link consistency analysis to detect non-canonical linking and redirect loops.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nimport { PageData, Issue, IssueSeverity } from '../types';\n\nexport const runPostCrawlAnalysis = (pages: PageData[]): PageData[] => {\n  const urlMap = new Map<string, PageData>();\n  const contentHashMap = new Map<string, string[]>(); \n  const inlinksMap = new Map<string, number>();\n\n  // 1. Build Indexes\n  pages.forEach(p => {\n    urlMap.set(p.url, p);\n    \n    // Fingerprint duplicate content\n    if (!contentHashMap.has(p.contentHash)) {\n      contentHashMap.set(p.contentHash, []);\n    }\n    contentHashMap.get(p.contentHash)?.push(p.url);\n\n    // Count basic inlinks (using object structure)\n    p.internalLinks.forEach(link => {\n        const currentCount = inlinksMap.get(link.url) || 0;\n        inlinksMap.set(link.url, currentCount + 1);\n    });\n  });\n\n  return pages.map(p => {\n    const newIssues = [...p.issues];\n\n    // A. Duplicate Content Clusters (Programmatic Issue)\n    const duplicates = contentHashMap.get(p.contentHash) || [];\n    if (duplicates.length > 1) {\n        if (p.canonical && p.canonical !== p.url) {\n             // Safe: Page is duplicate but canonicalized correctly\n        } else {\n             // Issue: Duplicate page without canonical pointing elsewhere\n             const others = duplicates.filter(d => d !== p.url).slice(0, 3).join(', ');\n             newIssues.push({\n                id: 'duplicate-content',\n                type: 'Content',\n                severity: IssueSeverity.HIGH,\n                message: 'Duplicate Content',\n                description: `Identical to ${duplicates.length - 1} other page(s): ${others}...`,\n                recommendation: 'Use canonical tags or 301 redirects.'\n             });\n        }\n    }\n\n    // B. Link Consistency & Intelligence (Deep Analysis)\n    p.internalLinks.forEach(link => {\n        const targetPage = urlMap.get(link.url);\n        \n        if (targetPage) {\n            // 1. Linking to Redirects (Redirect Loop Risk)\n            if (targetPage.status >= 300 && targetPage.status < 400) {\n                 newIssues.push({\n                    id: 'link-to-redirect',\n                    type: 'Links',\n                    severity: IssueSeverity.MEDIUM,\n                    message: 'Link to Redirect',\n                    description: `Anchor \"${link.text}\" points to ${link.url} which redirects (Status ${targetPage.status}).`,\n                    recommendation: 'Update internal link to point directly to the final destination.'\n                 });\n            }\n\n            // 2. Linking to Non-Canonical Version (Pin-point consistency)\n            // Determine the \"True Identity\" of the target page\n            const targetCanonical = targetPage.canonical || targetPage.url;\n            \n            // Check if link matches the target's declared canonical\n            // We ignore trailing slash differences if they are handled by the server, but strict SEO prefers exact match\n            if (targetCanonical && link.url !== targetCanonical) {\n                // If the target explicitly says \"My canonical is X\", but we linked to \"Y\" (where Y is this page's URL found by crawler)\n                // This usually implies Y is a duplicate version or parameterized version of X\n                \n                // Avoid flagging if the target IS the canonical but we just have a small variance (handled by normalization?)\n                // Actually, if we are here, link.url matched targetPage.url EXACTLY in the map.\n                // So if targetPage.canonical !== targetPage.url, then targetPage is non-canonical.\n                \n                newIssues.push({\n                    id: 'link-inconsistent',\n                    type: 'Links',\n                    severity: IssueSeverity.HIGH,\n                    message: 'Linking to Non-Canonical',\n                    description: `Anchor \"${link.text}\" links to non-canonical version. Target: ${link.url}. Target declares canonical as: ${targetCanonical}.`,\n                    recommendation: `Change link href to: ${targetCanonical}`\n                });\n            }\n        }\n    });\n\n    // C. Orphan Pages\n    const inlinks = inlinksMap.get(p.url) || 0;\n    if (inlinks === 0 && p.depth > 0) {\n        newIssues.push({\n            id: 'orphan-page',\n            type: 'Structure',\n            severity: IssueSeverity.HIGH,\n            message: 'Orphan Page',\n            description: 'No internal links point to this page.',\n            recommendation: 'Link from relevant content or sitemap.'\n        });\n    }\n\n    // D. Internal Link Score\n    const inRank = Math.min(10, Math.ceil((inlinks / Math.max(1, pages.length * 0.1)) * 10));\n\n    return {\n        ...p,\n        issues: newIssues,\n        inlinksCount: inlinks,\n        inRank\n    };\n  });\n};\n"
            }
          ]
        },
        {
          "path": "App.tsx",
          "generationType": 1,
          "isDone": true,
          "description": "Add Link Consistency view to dashboard.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nimport React, { useState, useCallback, useRef, useEffect, useMemo } from 'react';\nimport { Play, Pause, Square, Settings, Search, AlertCircle, FileText, Globe, Link as LinkIcon, Download, Layers, Activity, Braces, ChevronDown, ChevronRight, CornerDownRight, X, GitMerge } from 'lucide-react';\nimport { Crawler } from './services/crawler';\nimport { runPostCrawlAnalysis } from './services/postAnalysis';\nimport { PageData, CrawlSettings, DEFAULT_SETTINGS, CrawlStats, IssueSeverity } from './types';\nimport { Dashboard } from './components/Dashboard';\nimport { IssueDetails } from './components/IssueDetails';\n\nconst App: React.FC = () => {\n  const [urlInput, setUrlInput] = useState('https://example.com');\n  const [pages, setPages] = useState<PageData[]>([]);\n  const [stats, setStats] = useState<CrawlStats>({\n    pagesCrawled: 0,\n    queueLength: 0,\n    currentDepth: 0,\n    startTime: Date.now(),\n    endTime: null,\n    status: 'IDLE',\n  });\n  const [selectedPage, setSelectedPage] = useState<PageData | null>(null);\n  const [settings, setSettings] = useState<CrawlSettings>(DEFAULT_SETTINGS);\n  const [showSettings, setShowSettings] = useState(false);\n  const [filter, setFilter] = useState('');\n  \n  const crawlerRef = useRef<Crawler | null>(null);\n\n  // Tab & Filter State\n  const [activeTab, setActiveTab] = useState<'ALL' | 'ISSUES' | 'CANONICALS' | 'IMAGES' | 'SCHEMA' | 'LINKS'>('ALL');\n  const [isIssuesExpanded, setIsIssuesExpanded] = useState(false);\n  const [selectedIssueFilter, setSelectedIssueFilter] = useState<string | null>(null);\n\n  const handlePageCrawled = useCallback((page: PageData) => {\n    setPages(prev => {\n        if (prev.find(p => p.url === page.url)) return prev;\n        return [...prev, page];\n    });\n    setStats(prev => ({\n        ...prev,\n        pagesCrawled: prev.pagesCrawled + 1,\n    }));\n  }, []);\n\n  const handleComplete = useCallback(() => {\n    setStats(prev => ({ ...prev, status: 'ANALYZING' }));\n    setTimeout(() => {\n        setPages(currentPages => runPostCrawlAnalysis(currentPages));\n        setStats(prev => ({ ...prev, status: 'COMPLETED', endTime: Date.now() }));\n    }, 100);\n  }, []);\n\n  const startCrawl = () => {\n    if (!urlInput) return;\n    setPages([]);\n    setSelectedIssueFilter(null);\n    setStats({ pagesCrawled: 0, queueLength: 0, currentDepth: 0, startTime: Date.now(), endTime: null, status: 'CRAWLING' });\n    crawlerRef.current = new Crawler(settings, handlePageCrawled, handleComplete);\n    crawlerRef.current.start(urlInput);\n  };\n\n  const stopCrawl = () => {\n    if (crawlerRef.current) crawlerRef.current.stop();\n  };\n\n  // Calculate distinct issues and their counts\n  const issueGroups = useMemo(() => {\n    const counts: Record<string, number> = {};\n    pages.forEach(p => {\n        p.issues.forEach(i => {\n            counts[i.message] = (counts[i.message] || 0) + 1;\n        });\n    });\n    return Object.entries(counts).sort((a, b) => b[1] - a[1]); // Sort by frequency\n  }, [pages]);\n\n  const filteredPages = pages.filter(p => {\n    const matchesSearch = p.url.toLowerCase().includes(filter.toLowerCase()) || \n                          p.title?.toLowerCase().includes(filter.toLowerCase());\n    if (!matchesSearch) return false;\n\n    if (activeTab === 'ISSUES') {\n        if (selectedIssueFilter) {\n            return p.issues.some(i => i.message === selectedIssueFilter);\n        }\n        return p.issues.length > 0;\n    }\n    if (activeTab === 'CANONICALS') return p.canonical && p.canonical !== p.url;\n    if (activeTab === 'IMAGES') return p.images.length > 0;\n    if (activeTab === 'SCHEMA') return p.schemas.length > 0;\n    if (activeTab === 'LINKS') return p.issues.some(i => i.id === 'link-inconsistent' || i.id === 'link-to-redirect');\n    \n    return true;\n  });\n\n  const exportCSV = () => {\n      const headers = ['URL', 'Status', 'InRank', 'Depth', 'Title', 'Schema Count', 'Image Issues', 'Issues Count', 'Critical Issues'];\n      const rows = pages.map(p => [\n          p.url,\n          p.status,\n          p.inRank,\n          p.depth,\n          `\"${(p.title || '').replace(/\"/g, '\"\"')}\"`,\n          p.schemas.length,\n          p.issues.filter(i => i.type === 'Images').length,\n          p.issues.length,\n          `\"${p.issues.filter(i => i.severity === IssueSeverity.CRITICAL || i.severity === IssueSeverity.HIGH).map(i => i.message).join('; ')}\"`\n      ]);\n      const csvContent = \"data:text/csv;charset=utf-8,\" + [headers.join(','), ...rows.map(r => r.join(','))].join('\\n');\n      const link = document.createElement(\"a\");\n      link.setAttribute(\"href\", encodeURI(csvContent));\n      link.setAttribute(\"download\", `aurora_audit_${Date.now()}.csv`);\n      document.body.appendChild(link);\n      link.click();\n      document.body.removeChild(link);\n  };\n\n  return (\n    <div className=\"min-h-screen flex flex-col bg-slate-950 text-slate-200 font-sans selection:bg-indigo-500/30\">\n      <header className=\"h-16 border-b border-slate-800 bg-slate-950 flex items-center px-6 justify-between sticky top-0 z-40 shadow-lg shadow-black/20\">\n        <div className=\"flex items-center gap-3\">\n          <div className=\"w-8 h-8 bg-indigo-600 rounded flex items-center justify-center shadow-lg shadow-indigo-500/20\"><Layers className=\"text-white w-5 h-5\" /></div>\n          <h1 className=\"text-xl font-bold tracking-tight text-white\">AURORA<span className=\"text-indigo-500\">-X</span></h1>\n        </div>\n        <div className=\"flex-1 max-w-2xl mx-8 flex gap-2\">\n          <div className=\"relative flex-1 group\">\n             <div className=\"absolute inset-y-0 left-0 pl-3 flex items-center pointer-events-none\"><Globe className=\"h-4 w-4 text-slate-500 group-focus-within:text-indigo-400 transition-colors\" /></div>\n             <input type=\"text\" className=\"w-full bg-slate-900 border border-slate-700 rounded-md pl-10 pr-4 py-2 text-sm focus:ring-1 focus:ring-indigo-500 focus:border-indigo-500 outline-none transition-all placeholder:text-slate-600\" placeholder=\"Enter URL (Stock site or Programmatic SEO site)...\" value={urlInput} onChange={(e) => setUrlInput(e.target.value)} />\n          </div>\n          {stats.status === 'CRAWLING' ? (\n             <button onClick={stopCrawl} className=\"bg-red-600 hover:bg-red-700 text-white px-4 py-2 rounded-md flex items-center text-sm font-medium transition-colors shadow-lg shadow-red-900/20\"><Square className=\"w-4 h-4 mr-2 fill-current\" /> Stop</button>\n          ) : (\n             <button onClick={startCrawl} className=\"bg-indigo-600 hover:bg-indigo-700 text-white px-6 py-2 rounded-md flex items-center text-sm font-medium transition-colors shadow-lg shadow-indigo-900/20\"><Play className=\"w-4 h-4 mr-2 fill-current\" /> Start Audit</button>\n          )}\n          <button onClick={() => setShowSettings(!showSettings)} className={`p-2 rounded-md border border-slate-700 hover:bg-slate-800 transition-colors ${showSettings ? 'bg-slate-800 text-indigo-400 border-indigo-500/50' : 'text-slate-400'}`}><Settings className=\"w-5 h-5\" /></button>\n        </div>\n        <div className=\"flex items-center gap-4 text-xs font-mono text-slate-500\">\n          <div><span className=\"text-slate-400\">Pages:</span> {stats.pagesCrawled}</div>\n          <div><span className=\"text-slate-400\">Status:</span> <span className={`ml-2 px-2 py-0.5 rounded ${stats.status === 'CRAWLING' ? 'bg-indigo-500/10 text-indigo-400 animate-pulse' : stats.status === 'COMPLETED' ? 'bg-green-500/10 text-green-400' : 'bg-slate-800'}`}>{stats.status}</span></div>\n        </div>\n      </header>\n      {showSettings && (\n        <div className=\"bg-slate-900 border-b border-slate-800 p-6 animate-in slide-in-from-top-2\">\n           <div className=\"max-w-4xl mx-auto grid grid-cols-1 md:grid-cols-3 gap-6\">\n              <div><label className=\"block text-xs font-semibold text-slate-400 uppercase mb-2\">CORS Proxy</label><input type=\"text\" className=\"w-full bg-slate-950 border border-slate-700 rounded px-3 py-2 text-sm text-slate-200\" value={settings.proxyUrl} onChange={(e) => setSettings({...settings, proxyUrl: e.target.value})} placeholder=\"https://corsproxy.io/?\" /><p className=\"text-[10px] text-slate-500 mt-1\">Required for crawling.</p></div>\n              <div><label className=\"block text-xs font-semibold text-slate-400 uppercase mb-2\">Max Pages</label><input type=\"number\" className=\"w-full bg-slate-950 border border-slate-700 rounded px-3 py-2 text-sm text-slate-200\" value={settings.maxPages} onChange={(e) => setSettings({...settings, maxPages: parseInt(e.target.value)})} /></div>\n              <div><label className=\"block text-xs font-semibold text-slate-400 uppercase mb-2\">Max Depth</label><input type=\"number\" className=\"w-full bg-slate-950 border border-slate-700 rounded px-3 py-2 text-sm text-slate-200\" value={settings.maxDepth} onChange={(e) => setSettings({...settings, maxDepth: parseInt(e.target.value)})} /></div>\n           </div>\n        </div>\n      )}\n      <main className=\"flex-1 flex overflow-hidden\">\n        <nav className=\"w-16 md:w-64 flex-shrink-0 bg-slate-950 border-r border-slate-800 flex flex-col overflow-y-auto\">\n            <div className=\"p-4 space-y-2\">\n                <button onClick={() => { setActiveTab('ALL'); setSelectedIssueFilter(null); }} className={`w-full flex items-center gap-3 px-3 py-2 rounded-md text-sm transition-colors ${activeTab === 'ALL' ? 'bg-indigo-600/10 text-indigo-400' : 'text-slate-400 hover:bg-slate-900 hover:text-white'}`}><FileText className=\"w-4 h-4\" /><span className=\"hidden md:inline\">All URLs</span></button>\n                \n                {/* Issues Dropdown */}\n                <div>\n                    <button \n                        onClick={() => setIsIssuesExpanded(!isIssuesExpanded)} \n                        className={`w-full flex items-center justify-between px-3 py-2 rounded-md text-sm transition-colors ${activeTab === 'ISSUES' ? 'bg-indigo-600/10 text-indigo-400' : 'text-slate-400 hover:bg-slate-900 hover:text-white'}`}\n                    >\n                        <div className=\"flex items-center gap-3\">\n                            <AlertCircle className=\"w-4 h-4\" />\n                            <span className=\"hidden md:inline\">Issues Detected</span>\n                        </div>\n                        {isIssuesExpanded ? <ChevronDown className=\"w-4 h-4\" /> : <ChevronRight className=\"w-4 h-4\" />}\n                    </button>\n                    \n                    {isIssuesExpanded && (\n                        <div className=\"mt-1 ml-4 space-y-1 border-l border-slate-800 pl-2\">\n                             <button \n                                onClick={() => { setActiveTab('ISSUES'); setSelectedIssueFilter(null); }}\n                                className={`w-full flex items-center gap-2 px-3 py-1.5 rounded-md text-xs text-left transition-colors ${activeTab === 'ISSUES' && selectedIssueFilter === null ? 'text-white bg-slate-800' : 'text-slate-500 hover:text-slate-300'}`}\n                             >\n                                <CornerDownRight className=\"w-3 h-3 opacity-50\" /> All Issues\n                             </button>\n                             {issueGroups.map(([issueMessage, count]) => (\n                                <button \n                                    key={issueMessage}\n                                    onClick={() => { setActiveTab('ISSUES'); setSelectedIssueFilter(issueMessage); }}\n                                    className={`w-full flex items-center justify-between px-3 py-1.5 rounded-md text-xs text-left transition-colors ${selectedIssueFilter === issueMessage ? 'text-red-400 bg-red-950/20' : 'text-slate-500 hover:text-slate-300'}`}\n                                >\n                                   <span className=\"truncate pr-2\">{issueMessage}</span>\n                                   <span className=\"bg-slate-800 text-slate-400 px-1.5 py-0.5 rounded text-[10px] min-w-[20px] text-center\">{count}</span>\n                                </button>\n                             ))}\n                             {issueGroups.length === 0 && (\n                                <div className=\"px-3 py-2 text-[10px] text-slate-600 italic\">No issues found yet</div>\n                             )}\n                        </div>\n                    )}\n                </div>\n\n                <button onClick={() => { setActiveTab('LINKS'); setSelectedIssueFilter(null); }} className={`w-full flex items-center gap-3 px-3 py-2 rounded-md text-sm transition-colors ${activeTab === 'LINKS' ? 'bg-indigo-600/10 text-indigo-400' : 'text-slate-400 hover:bg-slate-900 hover:text-white'}`}><GitMerge className=\"w-4 h-4\" /><span className=\"hidden md:inline\">Link Consistency</span></button>\n                <button onClick={() => { setActiveTab('CANONICALS'); setSelectedIssueFilter(null); }} className={`w-full flex items-center gap-3 px-3 py-2 rounded-md text-sm transition-colors ${activeTab === 'CANONICALS' ? 'bg-indigo-600/10 text-indigo-400' : 'text-slate-400 hover:bg-slate-900 hover:text-white'}`}><LinkIcon className=\"w-4 h-4\" /><span className=\"hidden md:inline\">Canonicals</span></button>\n                <button onClick={() => { setActiveTab('IMAGES'); setSelectedIssueFilter(null); }} className={`w-full flex items-center gap-3 px-3 py-2 rounded-md text-sm transition-colors ${activeTab === 'IMAGES' ? 'bg-indigo-600/10 text-indigo-400' : 'text-slate-400 hover:bg-slate-900 hover:text-white'}`}><Activity className=\"w-4 h-4\" /><span className=\"hidden md:inline\">Image SEO</span></button>\n                <button onClick={() => { setActiveTab('SCHEMA'); setSelectedIssueFilter(null); }} className={`w-full flex items-center gap-3 px-3 py-2 rounded-md text-sm transition-colors ${activeTab === 'SCHEMA' ? 'bg-indigo-600/10 text-indigo-400' : 'text-slate-400 hover:bg-slate-900 hover:text-white'}`}><Braces className=\"w-4 h-4\" /><span className=\"hidden md:inline\">Schema Data</span></button>\n            </div>\n            <div className=\"mt-auto p-4\"><button onClick={exportCSV} className=\"w-full flex items-center justify-center gap-2 border border-slate-700 text-slate-400 hover:text-white hover:bg-slate-800 py-2 rounded-md text-sm transition-colors\"><Download className=\"w-4 h-4\" /><span className=\"hidden md:inline\">Export CSV</span></button></div>\n        </nav>\n        <div className=\"flex-1 overflow-y-auto p-6 bg-slate-950 relative\">\n            {pages.length > 0 && <Dashboard data={pages} />}\n            <div className=\"flex items-center justify-between mb-4\">\n               <div className=\"relative w-64\"><Search className=\"absolute left-3 top-1/2 -translate-y-1/2 w-4 h-4 text-slate-500\" /><input type=\"text\" placeholder=\"Filter URLs...\" className=\"w-full bg-slate-900 border border-slate-800 rounded-full pl-9 pr-4 py-1.5 text-sm focus:border-indigo-500 outline-none\" value={filter} onChange={(e) => setFilter(e.target.value)} /></div>\n               <div className=\"flex items-center gap-3\">\n                    {selectedIssueFilter && (\n                         <div className=\"flex items-center text-xs bg-red-900/20 text-red-400 px-3 py-1 rounded-full border border-red-900/50\">\n                             <AlertCircle className=\"w-3 h-3 mr-2\" />\n                             Filtered by: <span className=\"font-bold ml-1\">{selectedIssueFilter}</span>\n                             <button onClick={() => setSelectedIssueFilter(null)} className=\"ml-2 hover:text-white\"><X className=\"w-3 h-3\" /></button>\n                         </div>\n                    )}\n                    <div className=\"text-sm text-slate-400\">Showing {filteredPages.length} URLs</div>\n               </div>\n            </div>\n            <div className=\"border border-slate-800 rounded-lg overflow-hidden bg-slate-900 shadow-sm\">\n                <div className=\"overflow-x-auto\">\n                    <table className=\"w-full text-left text-sm whitespace-nowrap\">\n                        <thead className=\"bg-slate-950 text-slate-400 border-b border-slate-800 font-medium\">\n                            <tr>\n                                <th className=\"px-4 py-3 w-16 text-center\">InRank</th>\n                                <th className=\"px-4 py-3\">URL</th>\n                                <th className=\"px-4 py-3 w-48\">Page Title</th>\n                                <th className=\"px-4 py-3 w-24 text-center\">Schema</th>\n                                <th className=\"px-4 py-3 w-32 text-center\">Issues</th>\n                                <th className=\"px-4 py-3 w-24\">Actions</th>\n                            </tr>\n                        </thead>\n                        <tbody className=\"divide-y divide-slate-800/50\">\n                            {filteredPages.map((page) => (\n                                <tr key={page.url} className={`hover:bg-slate-800/50 transition-colors ${selectedPage?.url === page.url ? 'bg-indigo-900/20' : ''}`}>\n                                    <td className=\"px-4 py-3 text-center\"><div className=\"flex justify-center\"><div className={`w-6 h-6 rounded-full flex items-center justify-center text-[10px] font-bold ${page.inRank >= 8 ? 'bg-green-500 text-slate-950' : page.inRank >= 4 ? 'bg-blue-500 text-white' : 'bg-slate-700 text-slate-400'}`}>{page.inRank}</div></div></td>\n                                    <td className=\"px-4 py-3 max-w-md truncate text-slate-300\" title={page.url}>{page.url}</td>\n                                    <td className=\"px-4 py-3 max-w-xs truncate text-slate-400\" title={page.title || ''}>{page.title || <span className=\"text-slate-600 italic\">Missing</span>}</td>\n                                    <td className=\"px-4 py-3 text-center text-slate-400 font-mono\">{page.schemas.length > 0 ? <span className=\"text-green-400\">{page.schemas.length}</span> : '-'}</td>\n                                    <td className=\"px-4 py-3 text-center\">{page.issues.length > 0 ? <span className={`px-2 py-0.5 rounded-full text-xs font-bold ${page.issues.some(i => i.severity === IssueSeverity.CRITICAL) ? 'bg-red-500/20 text-red-500' : 'bg-slate-700 text-slate-300'}`}>{page.issues.length}</span> : <span className=\"text-slate-600\">-</span>}</td>\n                                    <td className=\"px-4 py-3\"><button onClick={() => setSelectedPage(page)} className=\"text-indigo-400 hover:text-indigo-300 text-xs font-medium\">Inspect</button></td>\n                                </tr>\n                            ))}\n                            {filteredPages.length === 0 && (\n                                <tr>\n                                    <td colSpan={6} className=\"px-4 py-8 text-center text-slate-500\">\n                                        No URLs matching current filters.\n                                    </td>\n                                </tr>\n                            )}\n                        </tbody>\n                    </table>\n                </div>\n            </div>\n        </div>\n        {selectedPage && <IssueDetails page={selectedPage} onClose={() => setSelectedPage(null)} />}\n      </main>\n    </div>\n  );\n};\n\nexport default App;\n"
            }
          ]
        },
        {
          "path": "components/IssueDetails.tsx",
          "generationType": 1,
          "isDone": true,
          "description": "Add Outgoing Links tab to detail view to help pinpoint link issues.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nimport React, { useState } from 'react';\nimport { PageData, Issue, IssueSeverity } from '../types';\nimport { AlertTriangle, AlertOctagon, Info, CheckCircle, X, Sparkles, Key, Code, Eye, FileText, Share2, Image as ImageIcon, Braces, Link as LinkIcon } from 'lucide-react';\nimport { GoogleGenAI } from \"@google/genai\";\n\ninterface IssueDetailsProps {\n  page: PageData;\n  onClose: () => void;\n}\n\nconst SeverityIcon = ({ severity }: { severity: IssueSeverity }) => {\n  switch (severity) {\n    case IssueSeverity.CRITICAL: return <AlertOctagon className=\"w-5 h-5 text-red-500\" />;\n    case IssueSeverity.HIGH: return <AlertTriangle className=\"w-5 h-5 text-orange-500\" />;\n    case IssueSeverity.MEDIUM: return <AlertTriangle className=\"w-5 h-5 text-yellow-500\" />;\n    case IssueSeverity.LOW: return <Info className=\"w-5 h-5 text-blue-500\" />;\n    default: return <Info className=\"w-5 h-5 text-slate-400\" />;\n  }\n};\n\nexport const IssueDetails: React.FC<IssueDetailsProps> = ({ page, onClose }) => {\n  const [activeTab, setActiveTab] = useState<'ISSUES' | 'IMAGES' | 'SCHEMA' | 'LINKS'>('ISSUES');\n  const [aiAnalysis, setAiAnalysis] = useState<string | null>(null);\n  const [loadingAi, setLoadingAi] = useState(false);\n  const [apiKey, setApiKey] = useState(localStorage.getItem('gemini_api_key') || '');\n  const [showKeyInput, setShowKeyInput] = useState(false);\n\n  const handleAiFix = async (issue: Issue) => {\n    if (!apiKey) {\n      setShowKeyInput(true);\n      return;\n    }\n    setLoadingAi(true);\n    try {\n      const ai = new GoogleGenAI({ apiKey });\n      const prompt = `SEO Technical Audit. URL: ${page.url}. Issue: ${issue.message} (${issue.description}). Provide technical fix code.`;\n      const response = await ai.models.generateContent({ model: 'gemini-2.5-flash-latest', contents: prompt });\n      setAiAnalysis(response.text);\n    } catch (error) {\n      setAiAnalysis(\"Error generating AI fix.\");\n    } finally {\n      setLoadingAi(false);\n    }\n  };\n\n  const saveKey = (key: string) => {\n    setApiKey(key);\n    localStorage.setItem('gemini_api_key', key);\n    setShowKeyInput(false);\n  }\n\n  return (\n    <div className=\"fixed inset-y-0 right-0 w-full md:w-[700px] bg-slate-900 border-l border-slate-700 shadow-2xl transform transition-transform duration-300 overflow-y-auto z-50 flex flex-col\">\n      <div className=\"p-6 pb-0 flex-shrink-0\">\n        <div className=\"flex justify-between items-start mb-4\">\n          <div>\n            <div className=\"flex items-center gap-2 mb-1\">\n                 <h2 className=\"text-2xl font-bold text-white\">Audit Details</h2>\n                 <span className={`text-xs px-2 py-0.5 rounded font-bold ${page.inRank >= 5 ? 'bg-green-500 text-slate-900' : 'bg-slate-700 text-slate-300'}`}>InRank: {page.inRank}</span>\n            </div>\n            <p className=\"text-slate-400 text-sm break-all font-mono\">{page.url}</p>\n          </div>\n          <button onClick={onClose} className=\"p-2 hover:bg-slate-800 rounded-full transition-colors text-slate-400 hover:text-white\">\n            <X className=\"w-6 h-6\" />\n          </button>\n        </div>\n        \n        {/* Tabs */}\n        <div className=\"flex border-b border-slate-800 overflow-x-auto\">\n            <button \n              onClick={() => setActiveTab('ISSUES')} \n              className={`px-4 py-2 text-sm font-medium border-b-2 transition-colors ${activeTab === 'ISSUES' ? 'border-indigo-500 text-indigo-400' : 'border-transparent text-slate-400 hover:text-white'}`}\n            >\n                Issues ({page.issues.length})\n            </button>\n            <button \n              onClick={() => setActiveTab('LINKS')} \n              className={`px-4 py-2 text-sm font-medium border-b-2 transition-colors flex items-center gap-2 ${activeTab === 'LINKS' ? 'border-indigo-500 text-indigo-400' : 'border-transparent text-slate-400 hover:text-white'}`}\n            >\n                <LinkIcon className=\"w-3 h-3\"/> Links ({page.internalLinks.length})\n            </button>\n            <button \n              onClick={() => setActiveTab('IMAGES')} \n              className={`px-4 py-2 text-sm font-medium border-b-2 transition-colors flex items-center gap-2 ${activeTab === 'IMAGES' ? 'border-indigo-500 text-indigo-400' : 'border-transparent text-slate-400 hover:text-white'}`}\n            >\n                <ImageIcon className=\"w-3 h-3\"/> Images ({page.images.length})\n            </button>\n            <button \n              onClick={() => setActiveTab('SCHEMA')} \n              className={`px-4 py-2 text-sm font-medium border-b-2 transition-colors flex items-center gap-2 ${activeTab === 'SCHEMA' ? 'border-indigo-500 text-indigo-400' : 'border-transparent text-slate-400 hover:text-white'}`}\n            >\n                <Braces className=\"w-3 h-3\"/> Schema ({page.schemas.length})\n            </button>\n        </div>\n      </div>\n\n      <div className=\"p-6 overflow-y-auto flex-1\">\n        {activeTab === 'ISSUES' && (\n            <div className=\"space-y-6\">\n                 {/* Metric Cards */}\n                <div className=\"grid grid-cols-4 gap-2 mb-2\">\n                    <div className=\"bg-slate-950 p-2 rounded border border-slate-800 flex flex-col items-center\">\n                        <span className=\"text-[10px] text-slate-500 uppercase\">Words</span>\n                        <span className=\"font-mono text-lg text-slate-200\">{page.wordCount}</span>\n                    </div>\n                    <div className=\"bg-slate-950 p-2 rounded border border-slate-800 flex flex-col items-center\">\n                        <span className=\"text-[10px] text-slate-500 uppercase\">DOM</span>\n                        <span className={`font-mono text-lg ${page.domNodeCount > 1500 ? 'text-orange-400' : 'text-slate-200'}`}>{page.domNodeCount}</span>\n                    </div>\n                    <div className=\"bg-slate-950 p-2 rounded border border-slate-800 flex flex-col items-center\">\n                        <span className=\"text-[10px] text-slate-500 uppercase\">Inlinks</span>\n                        <span className=\"font-mono text-lg text-slate-200\">{page.inlinksCount}</span>\n                    </div>\n                    <div className=\"bg-slate-950 p-2 rounded border border-slate-800 flex flex-col items-center\">\n                        <span className=\"text-[10px] text-slate-500 uppercase\">Load</span>\n                        <span className={`font-mono text-lg ${page.loadTime > 1000 ? 'text-red-400':'text-green-400'}`}>{page.loadTime}ms</span>\n                    </div>\n                </div>\n\n                {/* AI Key */}\n                {showKeyInput && (\n                <div className=\"bg-indigo-900/20 p-4 rounded-lg mb-4 border border-indigo-500/50\">\n                    <input type=\"password\" placeholder=\"Gemini API Key...\" className=\"w-full bg-slate-950 border border-slate-700 rounded px-3 py-2 text-sm text-white mb-2\" onChange={(e) => setApiKey(e.target.value)} value={apiKey} />\n                    <button onClick={() => saveKey(apiKey)} className=\"text-xs bg-indigo-600 hover:bg-indigo-700 text-white px-3 py-1 rounded w-full\">Save Key</button>\n                </div>\n                )}\n\n                <div className=\"space-y-3\">\n                    {page.issues.length === 0 && <p className=\"text-slate-500 text-sm text-center py-8\">No issues found on this page.</p>}\n                    {page.issues.map((issue, idx) => (\n                    <div key={idx} className=\"bg-slate-950 border border-slate-800 rounded-lg p-4\">\n                        <div className=\"flex items-start gap-3\">\n                        <SeverityIcon severity={issue.severity} />\n                        <div className=\"flex-1\">\n                            <div className=\"flex justify-between items-start\">\n                            <h4 className=\"text-slate-200 font-medium text-sm\">{issue.message}</h4>\n                            <span className={`text-[10px] px-2 py-0.5 rounded font-bold uppercase tracking-wide opacity-80 ${issue.severity === IssueSeverity.CRITICAL ? 'bg-red-900 text-red-200' : issue.severity === IssueSeverity.HIGH ? 'bg-orange-900 text-orange-200' : 'bg-blue-900 text-blue-200'}`}>{issue.severity}</span>\n                            </div>\n                            <p className=\"text-xs text-slate-400 mt-1\">{issue.description}</p>\n                            <div className=\"mt-2 text-xs text-slate-500 border-l-2 border-slate-700 pl-2\">Fix: {issue.recommendation}</div>\n                            <button onClick={() => handleAiFix(issue)} disabled={loadingAi} className=\"mt-3 flex items-center text-[10px] text-indigo-400 hover:text-indigo-300\"><Sparkles className=\"w-3 h-3 mr-1\" />{loadingAi ? \"Thinking...\" : \"AI Fix Recommendation\"}</button>\n                            {aiAnalysis && !loadingAi && <div className=\"mt-2 p-3 bg-indigo-950/30 border border-indigo-500/30 rounded text-xs text-indigo-200 relative\"><X className=\"w-3 h-3 absolute top-2 right-2 cursor-pointer\" onClick={()=>setAiAnalysis(null)}/><pre className=\"whitespace-pre-wrap font-sans\">{aiAnalysis}</pre></div>}\n                        </div>\n                        </div>\n                    </div>\n                    ))}\n                </div>\n            </div>\n        )}\n\n        {activeTab === 'LINKS' && (\n             <div className=\"space-y-4\">\n                 <div className=\"text-xs text-slate-400 mb-2\">Detailed breakdown of outgoing internal links found on this page.</div>\n                 {page.internalLinks.length === 0 ? <p className=\"text-slate-500 text-sm\">No internal links found.</p> : (\n                    <div className=\"bg-slate-950 border border-slate-800 rounded-lg overflow-hidden\">\n                        <table className=\"w-full text-left text-xs\">\n                            <thead className=\"bg-slate-900 text-slate-400 font-medium\">\n                                <tr>\n                                    <th className=\"px-3 py-2\">Anchor Text</th>\n                                    <th className=\"px-3 py-2\">Target URL</th>\n                                </tr>\n                            </thead>\n                            <tbody className=\"divide-y divide-slate-800\">\n                                {page.internalLinks.map((link, i) => (\n                                    <tr key={i} className=\"hover:bg-slate-800/50\">\n                                        <td className=\"px-3 py-2 text-slate-300 font-medium truncate max-w-[150px]\" title={link.text}>{link.text}</td>\n                                        <td className=\"px-3 py-2 text-slate-400 font-mono truncate max-w-[200px]\" title={link.url}>{link.url}</td>\n                                    </tr>\n                                ))}\n                            </tbody>\n                        </table>\n                    </div>\n                 )}\n             </div>\n        )}\n\n        {activeTab === 'IMAGES' && (\n            <div className=\"space-y-4\">\n                <div className=\"text-xs text-slate-400 mb-2\">Analyzing {page.images.length} images for SEO tags, dimensions, and formats.</div>\n                {page.images.length === 0 ? <p className=\"text-slate-500\">No images found.</p> : (\n                    <div className=\"grid grid-cols-1 gap-3\">\n                        {page.images.map((img, i) => (\n                            <div key={i} className=\"bg-slate-950 border border-slate-800 p-3 rounded flex gap-3 items-start\">\n                                <div className=\"w-16 h-16 bg-slate-900 flex items-center justify-center rounded overflow-hidden flex-shrink-0\">\n                                    <img src={img.src} alt=\"\" className=\"max-w-full max-h-full opacity-50\" onError={(e) => (e.currentTarget.src = 'https://placehold.co/64x64?text=ERR')} />\n                                </div>\n                                <div className=\"flex-1 min-w-0\">\n                                    <div className=\"flex items-center gap-2\">\n                                        <p className=\"text-xs text-slate-300 font-mono truncate\">{img.src.split('/').pop()}</p>\n                                        {!img.src.match(/\\.(webp|avif)$/i) && !img.src.startsWith('data:') && <span className=\"text-[9px] bg-orange-900 text-orange-200 px-1 rounded\">LEGACY FMT</span>}\n                                        {img.src.startsWith('http:') && <span className=\"text-[9px] bg-red-900 text-red-200 px-1 rounded\">INSECURE</span>}\n                                    </div>\n                                    <div className=\"grid grid-cols-2 gap-2 mt-2\">\n                                        <div>\n                                            <span className=\"text-[10px] text-slate-500 block uppercase\">Alt Text</span>\n                                            {img.alt ? (\n                                                <span className={`text-xs ${img.alt.length > 100 ? 'text-orange-400' : 'text-slate-300'}`}>{img.alt}</span>\n                                            ) : (\n                                                <span className=\"text-xs text-red-500 font-bold flex items-center gap-1\"><AlertTriangle className=\"w-3 h-3\"/> Missing</span>\n                                            )}\n                                        </div>\n                                        <div>\n                                            <span className=\"text-[10px] text-slate-500 block uppercase\">Dimensions</span>\n                                            {img.width && img.height ? (\n                                                <span className=\"text-xs text-slate-300\">{img.width} x {img.height}</span>\n                                            ) : (\n                                                <span className=\"text-xs text-orange-500 flex items-center gap-1\"><AlertTriangle className=\"w-3 h-3\"/> Missing (CLS)</span>\n                                            )}\n                                        </div>\n                                    </div>\n                                </div>\n                            </div>\n                        ))}\n                    </div>\n                )}\n            </div>\n        )}\n\n        {activeTab === 'SCHEMA' && (\n             <div className=\"space-y-4\">\n                <div className=\"text-xs text-slate-400 mb-2\">Analyzing structured data (JSON-LD) for Programmatic SEO entities.</div>\n                {page.schemas.length === 0 ? (\n                    <div className=\"bg-slate-950 border border-slate-800 p-8 rounded text-center text-slate-500\">\n                        <Braces className=\"w-8 h-8 mx-auto mb-2 opacity-50\"/>\n                        <p>No JSON-LD Schema found.</p>\n                        <p className=\"text-xs mt-1 text-slate-600\">Consider adding BreadcrumbList, Article, or Product schema.</p>\n                    </div>\n                ) : (\n                    <div className=\"space-y-4\">\n                        {page.schemas.map((s, i) => (\n                            <div key={i} className={`bg-slate-950 border rounded-lg p-3 ${s.isValid ? 'border-slate-800' : 'border-red-900'}`}>\n                                <div className=\"flex justify-between items-center mb-2\">\n                                    <span className=\"text-sm font-bold text-indigo-400\">{s.type}</span>\n                                    {s.isValid ? <span className=\"text-[10px] bg-green-900 text-green-300 px-2 py-0.5 rounded\">VALID</span> : <span className=\"text-[10px] bg-red-900 text-red-200 px-2 py-0.5 rounded\">INVALID</span>}\n                                </div>\n                                {s.error && <div className=\"text-xs text-red-400 mb-2 font-mono bg-red-950/30 p-2 rounded\">{s.error}</div>}\n                                <pre className=\"text-[10px] text-slate-400 font-mono overflow-x-auto bg-slate-900 p-2 rounded\">\n                                    {s.raw}\n                                </pre>\n                            </div>\n                        ))}\n                    </div>\n                )}\n             </div>\n        )}\n\n      </div>\n    </div>\n  );\n};\n"
            }
          ]
        }
      ]
    },
    "createdTimestamp": {
      "seconds": 1770900847,
      "nanos": 730000000
    },
    "doneTimestamp": {
      "seconds": 1770900854,
      "nanos": 833000000
    }
  },
  {
    "id": 3,
    "author": "user",
    "payload": {
      "type": "text",
      "text": "Add a new check to `analyzePage` that identifies pages where internal links use different slugs or casing than the canonical URL. This should include pinpointing the linking page and the incorrect slug used.",
      "files": [],
      "fileUploads": [],
      "cssBlocks": [],
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770901098,
      "nanos": 894000000
    }
  },
  {
    "id": 4,
    "author": "model",
    "payload": {
      "type": "thinking",
      "text": "**Considering Internal Link Consistency**\n\nI'm currently focused on incorporating a check within either `analyzePage` or `runPostCrawlAnalysis` to catch internal link discrepancies. Specifically, I want to identify any inconsistencies in slugs or casing between internal links and the canonical URL of their target pages. I'm reviewing the existing implementation of `analyzePage` to understand where this check would best fit.\n\n\n**Refining Logic Location**\n\nI've decided that `runPostCrawlAnalysis` is the correct place for this check. The ability to compare linked URLs with their canonical versions necessitates the full crawl data, which is only available there. I'm focusing on the existing section in `postAnalysis.ts` to integrate this logic, specifically where link analysis is already happening. I'm reviewing the existing structure to best insert this check to ensure thoroughness and minimal performance impact.\n\n\n**Isolating Casing & Slug Issues**\n\nI'm now refining the discrepancy check to pinpoint casing and slug mismatches more precisely. Currently, any difference triggers the \"non-canonical\" check. I'm expanding the logic to identify cases where the only difference between a linked URL and its canonical target is casing or the slug. This will enable more specific error reporting, providing more helpful issue identification. I'll need to isolate the case and slug checks to create more specific error messages.\n\n\n**Differentiating Issue Types**\n\nI'm now implementing the differentiated issue types in `services/postAnalysis.ts`. Specifically, I'm enhancing `runPostCrawlAnalysis` to discern between casing and slug mismatches. I want to improve error reporting. I'll flag cases of solely casing mismatches as high priority. I want a more nuanced approach than the current \"non-canonical\" categorization, which I'll keep for cases with slug differences. I'm focusing on the current logic in `postAnalysis.ts`.\n\n\n**Prioritizing Trailing Slash**\n\nI am now adding the check for trailing slash mismatches. This is to catch wasted crawl budget. I'll make this a medium-priority issue. I'm focusing on incorporating this into the existing logic within `runPostCrawlAnalysis` and considering its place relative to the casing and slug discrepancy checks. I'll integrate it with the other checks for a comprehensive approach.\n\n\n**Implementing Casing Mismatch**\n\nI'm now implementing the casing check logic within the existing structure. I'm focusing on the condition where the lowercase pathnames match, but the full URLs don't. I'm prioritizing the exact location of the new issue type within `runPostCrawlAnalysis` to streamline issue reporting. I will return from the block after reporting a casing issue to avoid confusing potential further comparisons and to reduce performance impact.\n\n\n**Clarifying Target Retrieval**\n\nI'm now clarifying how `targetPage` is retrieved and its implications for the discrepancy checks. I've realized `targetPage` is fetched using the linked URL directly from the crawler data and will include the casing/slug that was followed, not the canonical. This means a direct comparison of `link.url !== targetPage.canonical` reveals inconsistencies. This has ramifications for my planned implementation.\n\n\n**Refining Analysis Scope**\n\nI'm now clarifying the scope of checks within `analyzePage`. While I cannot directly compare to other pages' canonicals during crawling, I can check if the current page is accessed via a non-canonical URL due to incoming links. I can flag a page if its URL, as found via a link, doesn't match the canonical on *that* page. This resolves the previous challenge. I'll focus on implementing this within `analyzePage` to catch this specific scenario.\n\n\n**Implementing Slug Verification**\n\nI'm now implementing the slug and casing mismatch checks in `postAnalysis.ts`. I'm confirming that the logic goes in `services/postAnalysis.ts`. I'm starting by identifying cases where internal links point to a page with a different slug or different casing. I will check for these discrepancies and report the mismatches. This involves comparing the linked URLs with their target pages' canonical URLs post-crawl. I'm focusing on parsing the slug from the URL.\n\n\n**Implementing Detailed Discrepancies**\n\nI'm now integrating detailed discrepancy checks in `services/postAnalysis.ts` within the `internalLinks.forEach` loop. I will retrieve the `targetPage` using the `link.url`, and then compare `link.url` to the `targetCanonical`. The previous \"Linking to Non-Canonical\" check will be replaced with precise error reporting. Specifically, if the URL casing is different, it will be labeled as \"Link Casing Mismatch\". Otherwise, if the slugs are different, it is labeled as \"Link Slug Mismatch / Non-Canonical\". The \"linking page\" is recorded as the key, allowing reporting on the dashboard to specify which anchor and page has the issue.\n\n\n**Refining URL Discrepancy Checks**\n\nI'm now integrating detailed discrepancy checks in `services/postAnalysis.ts` within the `internalLinks.forEach` loop. I will retrieve the `targetPage` using the `link.url`, and then compare `link.url` to the `targetCanonical`. The previous \"Linking to Non-Canonical\" check will be replaced with precise error reporting. Specifically, if the URL casing is different, it will be labeled as \"Link Casing Mismatch\". Otherwise, if the slugs are different, it is labeled as \"Link Slug Mismatch / Non-Canonical\". The \"linking page\" is recorded as the key, allowing reporting on the dashboard to specify which anchor and page has the issue.\n\n\n**Implementing Parameter Comparison**\n\nI'm now integrating the parameter comparison in `services/postAnalysis.ts`. I've added a check for URL parameters. I'll flag discrepancies. I have integrated `linkUrlObj.search !== targetUrlObj.search`. I'm working to make this more descriptive. I will include a better explanation in the description and recommendation for a mismatch.\n\n\n**Analyzing Crawler Interaction**\n\nI'm now focusing on how the crawler handles redirects and the implications for my discrepancy checks. Specifically, I'm examining how casing is preserved in the crawled URLs and the impact of redirects on internal link resolution. The crawler normalizes URLs but preserves casing from the `href` attribute. I'm considering scenarios where a link like `<a href=\"/Foo\">` leads to a redirect to `/foo`, and how the crawler captures this. I will look at the `redirected: true` status.\n\n\n**Examining Fetch Redirect Behavior**\n\nI'm now diving into the implications of how `fetch` handles redirects, which provides clarity on the discrepancy checks. By design, fetch transparently follows redirects, so the final URL is available in `response.url`. That, along with the crawler's normalization process, gives me the data I need to implement these detailed checks. I'm focusing on validating these assumptions and addressing the edge cases.\n\n\n**Confirming Auditor Integration**\n\nI'm now confirming the integration with `analyzePage` is unnecessary for the specified issue. I've clarified the user's request. Based on the description, I understand I should focus on the analysis done in `runPostCrawlAnalysis` to catch outgoing link discrepancies. I'm focusing on the implementation within `services/postAnalysis.ts`, where I'm extracting slugs and crafting specific error messages.\n\n\n**Verifying Casing Retention**\n\nI've verified `services/crawler.ts`. Specifically, the internal links preserve the raw URL casing found in `href`. The `URL` constructor's behavior in `services/crawler.ts` normalizes but maintains the casing. I'm satisfied with this behavior. I'm now refining the discrepancy checks in `services/postAnalysis.ts`.\n\n\n**Refining Error Reporting**\n\nI'm now refining the error reporting within the current logic in `postAnalysis.ts` to include the logic that I previously created. Specifically, I'm integrating the casing and slug mismatch checks with the newly defined issue types, which includes messages, descriptions and recommendations. I am ensuring that the correct data points are included in the messages, for example, the anchor text, the canonical, the target path, the link path. I'm focusing on ensuring these details are displayed correctly.\n\n\n**Addressing Null Canonical Scenarios**\n\nI'm now accounting for the scenario where a target page might lack a canonical tag, making `targetPage.canonical` null. The current implementation defaults to `targetPage.url` in this case. I'm focusing on ensuring accurate comparisons by handling this default. I'm evaluating the logic to confirm that `link.url` is compared against either the canonical or the page's URL if no canonical is specified. I'll verify the edge cases.\n\n\n**Validating Edge Cases**\n\nI'm now focusing on edge cases, particularly those involving redirects and missing canonical tags. After reviewing the crawler's behavior, I see how `targetPage.url` and `targetCanonical` are populated, allowing the crawler to handle the requested URL. I confirmed that casing mismatches are accurately identified even with redirects, thanks to how the crawler stores the original link. I'll focus on testing the null-canonical scenario.\n\n\n**Implementing Auditor Integration**\n\nI'm now integrating an additional check into `services/auditor.ts` as a secondary measure. While the core logic resides in `runPostCrawlAnalysis`, I'm adding a check for the current page's URL to verify that its access URL matches its canonical, even though this is less useful. I'm building on the slash normalization and the \"canonical\" issue categorization, as a secondary measure. I'm focusing on ensuring its seamless integration.\n\n\n**Refining Auditor Integration**\n\nI'm now refining the auditor integration to differentiate casing and slug mismatches for the current page. I'll modify `services/auditor.ts` to include these more specific checks, mirroring the logic in `postAnalysis.ts` but focused on the page's own URL. This will improve issue reporting for self-referential canonical problems. I'll need to modify the error messages to be more specific. I'm focusing on creating more specific error messaging.\n\n\n**Implementing Duplicate Fingerprinting**\n\nI'm now implementing the fingerprinting for duplicate content. Specifically, I'm integrating logic to use content hashes to identify potential duplicate pages. I'll flag pages as duplicates if their content hashes match and they lack a canonical tag pointing elsewhere, creating an issue. I'm focusing on ensuring these duplicates are efficiently identified within the existing page processing.\n\n\n**Implementing Duplicate Content Detection**\n\nI'm now integrating content fingerprinting within the crawler to flag potential duplicate content. I'm focusing on hashing content. I will raise an issue if a page matches the hash of another page and lacks a canonical tag, indicating a potential duplicate. I will provide details, and a high severity level. I'm ensuring it's seamlessly integrated.\n\n\n**Refining Parameter Comparisons**\n\nI'm now integrating detailed parameter comparisons to enhance the existing discrepancy checks, focusing on the `search` property of `URL` objects. Specifically, I'm differentiating URL mismatches. I'm modifying the implementation in `services/postAnalysis.ts` within the `internalLinks.forEach` loop to provide a more specific issue reporting for different parameters. I will adjust the description for clearer recommendations.\n\n\n**Integrating Parameter Validation**\n\nI'm now integrating comprehensive parameter validation into `services/postAnalysis.ts`. I will now provide more specific issue reports for parameter discrepancies found within internal links. Specifically, I am focusing on using the `search` property for URL comparisons. I'm updating descriptions to better articulate the issue and provide recommendations for resolution within the link checks.\n\n\n**Refining Duplicate Detection**\n\nI'm now refining the duplicate content detection by incorporating a stricter comparison. I'm focusing on handling situations where two pages have matching content hashes but different URLs. I'm ensuring that a more precise check is in place, and that the implementation correctly differentiates these cases. I'm focusing on creating more accurate duplicate detection.\n\n\n**Refining Internal Auditor Checks**\n\nI'm now refining the auditor integration to differentiate casing, slug, and parameter mismatches for the current page. I'll modify `services/auditor.ts` to include these more specific checks, mirroring the logic in `postAnalysis.ts` but focused on the page's own URL. I'm focusing on creating more specific error messaging.\n\n\n**Refining Parameter Comparisons**\n\nI'm now integrating enhanced parameter comparisons into the auditor and post-crawl analysis. I'm focusing on providing more descriptive and actionable insights for discrepancies in URL parameters within internal links and the canonical URL of the current page. I'm modifying and expanding the current checks within `services/auditor.ts` and `services/postAnalysis.ts`.\n\n\n**Enhancing Discrepancy Reporting**\n\nI've refined the discrepancy checks in `services/auditor.ts` and `services/postAnalysis.ts` to include specific error messages for casing, slug, and parameter mismatches. Specifically, I'm integrating enhanced parameter comparisons, leveraging the `search` property of `URL` objects. My focus remains on creating actionable, descriptive insights for these discrepancies. I've also integrated the logic that I previously created, and ensured it is working correctly.\n\n\n**Implementing Duplicate Content Detection**\n\nI'm now integrating logic for identifying duplicate content more comprehensively within the existing structure. I'm focusing on the current content hash implementation to flag pages. I have added logic to identify pages as duplicates if they share content hashes and lack a canonical pointing elsewhere. I'm focusing on using that to generate the proper issues.\n\n\n**Refining Content Duplicate Detection**\n\nI'm now integrating more advanced checks to identify pages with duplicate content. I'm focusing on ensuring duplicate content detection within `runPostCrawlAnalysis` to identify content that lacks canonical tags. I'm reviewing the recent implementation to provide more specific error reporting.\n\n\n**Integrating Enhanced Comparison**\n\nI've integrated enhanced parameter comparisons into the duplicate content checks. Now I will handle casing, slug, and parameter mismatches. I'm focusing on the integration of detailed comparison logic to create actionable insights for inconsistencies within internal links and canonical URLs. I'm ensuring all the recent adjustments are working as expected.\n\n\n**Integrating Parameter Comparisons**\n\nI'm now integrating comprehensive parameter comparison logic within the existing checks in `postAnalysis.ts` and `auditor.ts`. I will report parameter discrepancies. Specifically, I'm focusing on the `search` property to enhance issue reporting for mismatched URL parameters, and I'll include clearer, more actionable recommendations for resolution.\n\n\n**Enhancing Parameter Discrepancy Reporting**\n\nI'm now focusing on further refining parameter discrepancy reporting, focusing on more explicit error messages for internal link parameter mismatches and inconsistencies with canonical URLs. I'm refining how those discrepancies are reported. I will provide clearer, actionable insights to better guide users toward resolving parameter-related issues. I am also working to ensure that messages display relevant information correctly.\n\n\n**Refining Page Issues**\n\nI've integrated the generation of 'Orphan Page' issues when a page lacks internal links. I've also added an internal link score based on a percentage of the site. I will continue to focus on creating detailed issue types and improving overall structure. I'm focusing on the current implementation in `services/auditor.ts` and refining the issue types.\n\n\n**Refining Link Issue Severity**\n\nI'm now determining the severity level of casing, slug, and parameter mismatches based on their impact. I'll make casing mismatches medium, slug mismatches high, and parameter mismatches low. I'll focus on ensuring that this is an easy to adjust setting.\n\n\n**Categorizing Severity Levels**\n\nI'm now implementing the severity levels for link discrepancies. Casing mismatches are classified as medium-severity, slug mismatches as high, and parameter mismatches as low, as previously planned. I'm focusing on ensuring this aligns with overall issue reporting and making it configurable. I'm verifying the changes in `services/postAnalysis.ts` and `services/auditor.ts` for consistency.\n\n\n**Implementing Meta Data Analysis**\n\nI'm now integrating title and description meta data analysis. I've added checks for duplicate titles and H1s, keyword stuffing, missing descriptions, and long descriptions. I'm assigning severity levels to the issues. I'm working on including an `IssueSeverity` enum and focused on the appropriate issue details. I'll focus on ensuring that descriptions are within the correct length.\n\n\n**Refining Analysis Scope**\n\nI'm now integrating detailed meta data analysis as previously planned. I've incorporated checks for duplicate titles, H1s, keyword stuffing, and missing/long descriptions, assigning appropriate severity levels. Specifically, I'm focusing on ensuring accurate `IssueSeverity` and robust detail integration.\n\n\n**Refining Audit Severity**\n\nI'm now determining severity levels for title, H1, and description meta data issues based on their impact. I'll classify duplicate titles and H1s as high, and missing or long descriptions as medium. I'm focusing on consistency with the existing link issue severity classifications. I'm using an enum to categorize the priority.\n\n```text\nI'm focusing on ensuring these classifications accurately reflect their impact on SEO and user experience. This includes integrating the new severity levels with the `IssueSeverity` enum and verifying their correct assignment within `services/postAnalysis.ts` and `services/auditor.ts`. I am currently working on the specific logic in those files.\n```\n\n\n**Refining Hreflang Logic**\n\nI'm now refining the logic for hreflang checks, specifically addressing the self-referencing hreflang tag. I'm focusing on ensuring it correctly identifies instances where a page is missing its own hreflang tag, including handling cases with or without trailing slashes. I am ensuring that the `page.hreflangs` data is checked correctly.\n\n\n**Finalizing Severity Classification**\n\nI'm now finalizing the severity classifications. I'm focusing on providing more context within the analysis and testing thoroughly. I'll classify missing or malformed hreflang tags as a high-severity issue, mirroring the existing system. The implementation is almost complete.\n\n\n**Finalizing Content Analysis**\n\nI'm now integrating comprehensive content analysis. I'm focusing on incorporating the image and schema.org analysis into the existing system. This includes checks for insecure images, missing alt text, and legacy image formats, with varying severity levels. I am adding missing image attributes and schema verification as well. I'm focusing on ensuring robust detail integration.\n\n\n",
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770901098,
      "nanos": 894000000
    },
    "doneTimestamp": {
      "seconds": 1770901205,
      "nanos": 403000000
    }
  },
  {
    "id": 5,
    "author": "model",
    "payload": {
      "type": "generationTable",
      "entries": [
        {
          "path": "services/postAnalysis.ts",
          "generationType": 1,
          "isDone": true,
          "description": "Add deep link consistency checks for Casing and Slugs.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nimport { PageData, Issue, IssueSeverity } from '../types';\n\nexport const runPostCrawlAnalysis = (pages: PageData[]): PageData[] => {\n  const urlMap = new Map<string, PageData>();\n  const contentHashMap = new Map<string, string[]>(); \n  const inlinksMap = new Map<string, number>();\n\n  // 1. Build Indexes\n  pages.forEach(p => {\n    urlMap.set(p.url, p);\n    \n    // Fingerprint duplicate content\n    if (!contentHashMap.has(p.contentHash)) {\n      contentHashMap.set(p.contentHash, []);\n    }\n    contentHashMap.get(p.contentHash)?.push(p.url);\n\n    // Count basic inlinks (using object structure)\n    p.internalLinks.forEach(link => {\n        const currentCount = inlinksMap.get(link.url) || 0;\n        inlinksMap.set(link.url, currentCount + 1);\n    });\n  });\n\n  return pages.map(p => {\n    const newIssues = [...p.issues];\n\n    // A. Duplicate Content Clusters (Programmatic Issue)\n    const duplicates = contentHashMap.get(p.contentHash) || [];\n    if (duplicates.length > 1) {\n        if (p.canonical && p.canonical !== p.url) {\n             // Safe: Page is duplicate but canonicalized correctly\n        } else {\n             // Issue: Duplicate page without canonical pointing elsewhere\n             const others = duplicates.filter(d => d !== p.url).slice(0, 3).join(', ');\n             newIssues.push({\n                id: 'duplicate-content',\n                type: 'Content',\n                severity: IssueSeverity.HIGH,\n                message: 'Duplicate Content',\n                description: `Identical to ${duplicates.length - 1} other page(s): ${others}...`,\n                recommendation: 'Use canonical tags or 301 redirects.'\n             });\n        }\n    }\n\n    // B. Link Consistency & Intelligence (Deep Analysis)\n    p.internalLinks.forEach(link => {\n        const targetPage = urlMap.get(link.url);\n        \n        if (targetPage) {\n            // 1. Linking to Redirects (Redirect Loop Risk)\n            if (targetPage.status >= 300 && targetPage.status < 400) {\n                 newIssues.push({\n                    id: 'link-to-redirect',\n                    type: 'Links',\n                    severity: IssueSeverity.MEDIUM,\n                    message: 'Link to Redirect',\n                    description: `Anchor \"${link.text}\" points to ${link.url} which redirects (Status ${targetPage.status}).`,\n                    recommendation: 'Update internal link to point directly to the final destination.'\n                 });\n            }\n\n            // 2. Linking to Non-Canonical Version (Pin-point consistency)\n            const targetCanonical = targetPage.canonical || targetPage.url;\n            \n            if (targetCanonical && link.url !== targetCanonical) {\n                const linkObj = new URL(link.url);\n                const targetObj = new URL(targetCanonical);\n                \n                // Compare Pathnames (slugs) and Query Strings\n                const linkPath = decodeURIComponent(linkObj.pathname);\n                const targetPath = decodeURIComponent(targetObj.pathname);\n\n                if (linkPath !== targetPath) {\n                    if (linkPath.toLowerCase() === targetPath.toLowerCase()) {\n                         // Casing Mismatch\n                         newIssues.push({\n                            id: 'link-casing-mismatch',\n                            type: 'Links',\n                            severity: IssueSeverity.HIGH,\n                            message: 'Link Casing Mismatch',\n                            description: `Anchor \"${link.text}\" uses incorrect casing: \"${linkPath}\". Canonical is \"${targetPath}\".`,\n                            recommendation: `Update link href to match canonical casing: ${targetCanonical}`\n                         });\n                    } else {\n                         // Slug Mismatch\n                         newIssues.push({\n                            id: 'link-slug-mismatch',\n                            type: 'Links',\n                            severity: IssueSeverity.HIGH,\n                            message: 'Link Slug Mismatch',\n                            description: `Anchor \"${link.text}\" points to wrong slug: \"${linkPath}\". Canonical is \"${targetPath}\".`,\n                            recommendation: `Update link href to canonical URL: ${targetCanonical}`\n                         });\n                    }\n                } else if (linkObj.search !== targetObj.search) {\n                     // Query Params Mismatch\n                     newIssues.push({\n                        id: 'link-param-mismatch',\n                        type: 'Links',\n                        severity: IssueSeverity.MEDIUM,\n                        message: 'Link Parameter Mismatch',\n                        description: `Anchor \"${link.text}\" includes parameters not in canonical.`,\n                        recommendation: `Update link to canonical version: ${targetCanonical}`\n                     });\n                } else if (link.url.endsWith('/') !== targetCanonical.endsWith('/')) {\n                     // Trailing Slash\n                     newIssues.push({\n                        id: 'link-trailing-slash',\n                        type: 'Links',\n                        severity: IssueSeverity.MEDIUM,\n                        message: 'Trailing Slash Inconsistency',\n                        description: `Anchor \"${link.text}\" has slash mismatch with canonical.`,\n                        recommendation: `Update link to: ${targetCanonical}`\n                     });\n                }\n            }\n        }\n    });\n\n    // C. Orphan Pages\n    const inlinks = inlinksMap.get(p.url) || 0;\n    if (inlinks === 0 && p.depth > 0) {\n        newIssues.push({\n            id: 'orphan-page',\n            type: 'Structure',\n            severity: IssueSeverity.HIGH,\n            message: 'Orphan Page',\n            description: 'No internal links point to this page.',\n            recommendation: 'Link from relevant content or sitemap.'\n        });\n    }\n\n    // D. Internal Link Score\n    const inRank = Math.min(10, Math.ceil((inlinks / Math.max(1, pages.length * 0.1)) * 10));\n\n    return {\n        ...p,\n        issues: newIssues,\n        inlinksCount: inlinks,\n        inRank\n    };\n  });\n};\n"
            }
          ]
        },
        {
          "path": "services/auditor.ts",
          "generationType": 1,
          "isDone": true,
          "description": "Enhance analyzePage with specific checks for current URL casing vs canonical.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nimport { PageData, Issue, IssueSeverity } from '../types';\n\nexport const analyzePage = (page: PageData): Issue[] => {\n  const issues: Issue[] = [];\n  const addIssue = (\n    id: string, \n    type: string, \n    severity: IssueSeverity, \n    msg: string, \n    desc: string, \n    rec: string\n  ) => {\n    issues.push({ id, type, severity, message: msg, description: desc, recommendation: rec });\n  };\n\n  // --- 1. RESPONSE CODES ---\n  if (page.status === 0) {\n    addIssue('network-err', 'Response', IssueSeverity.CRITICAL, 'Network/Blocked', \n      'Crawler failed to access URL.', 'Check firewall, CORS, or server status.');\n  } else if (page.status >= 400 && page.status < 500) {\n    addIssue(`client-${page.status}`, 'Response', IssueSeverity.HIGH, `Client Error ${page.status}`, \n      `Page returned ${page.status}.`, 'Restore page or redirect.');\n  } else if (page.status >= 500) {\n    addIssue(`server-${page.status}`, 'Response', IssueSeverity.CRITICAL, `Server Error ${page.status}`, \n      'Server crashed processing request.', 'Check server logs.');\n  }\n\n  if (page.status !== 200) return issues;\n\n  // --- 2. URL & PROGRAMMATIC STRUCTURE ---\n  if (page.url.includes('_')) {\n    addIssue('url-underscore', 'URL', IssueSeverity.LOW, 'Underscores in URL', 'Google prefers hyphens.', 'Use hyphens (-) instead.');\n  }\n  if (/[A-Z]/.test(page.url)) {\n    addIssue('url-uppercase', 'URL', IssueSeverity.MEDIUM, 'Uppercase in URL', 'URLs are case-sensitive on many servers.', 'Lowercase all URLs.');\n  }\n  \n  // Programmatic: Parameter Check\n  if ((page.url.match(/\\?/g) || []).length > 0 && page.url.length > 100) {\n      addIssue('complex-params', 'URL', IssueSeverity.MEDIUM, 'Complex Query Parameters', 'URL is long and contains parameters.', 'Use cleaner URLs for programmatic pages.');\n  }\n\n  // --- 3. TITLE & DESCRIPTION ---\n  if (!page.title) {\n    addIssue('missing-title', 'Meta', IssueSeverity.HIGH, 'Missing Title', 'No <title> tag found.', 'Add a unique title.');\n  } else {\n    if (page.title.length < 10) addIssue('short-title', 'Meta', IssueSeverity.LOW, 'Title Too Short', `Title is ${page.title.length} chars.`, 'Aim for 30-60 chars.');\n    if (page.title.length > 60) addIssue('long-title', 'Meta', IssueSeverity.MEDIUM, 'Title Too Long', `Title is ${page.title.length} chars.`, 'Truncate below 60 chars.');\n    if (page.h1 && page.title === page.h1) addIssue('title-h1-dup', 'Meta', IssueSeverity.LOW, 'Title equals H1', 'Title and H1 are identical.', 'Optimize Title for SERP and H1 for user context.');\n    \n    // Keyword stuffing check\n    const words = page.title.split(' ');\n    const uniqueWords = new Set(words.map(w => w.toLowerCase()));\n    if (words.length > 5 && uniqueWords.size < words.length * 0.6) {\n         addIssue('title-stuffing', 'Meta', IssueSeverity.HIGH, 'Title Keyword Stuffing', 'Repetitive words detected.', 'Write natural titles.');\n    }\n  }\n\n  if (!page.description) {\n    addIssue('missing-desc', 'Meta', IssueSeverity.MEDIUM, 'Missing Meta Description', 'No description found.', 'Add meta description for CTR.');\n  } else if (page.description.length > 160) {\n    addIssue('long-desc', 'Meta', IssueSeverity.LOW, 'Description Too Long', `Length: ${page.description.length}.`, 'Keep under 160 chars.');\n  }\n\n  // --- 4. CONTENT QUALITY ---\n  if (page.wordCount < 200) {\n    addIssue('thin-content', 'Content', IssueSeverity.HIGH, 'Thin Content', `Only ${page.wordCount} words found.`, 'Add more substantial content to rank.');\n  }\n  if (page.textRatio < 10) {\n    addIssue('low-text-ratio', 'Content', IssueSeverity.LOW, 'Low Text-to-HTML Ratio', `Ratio is ${page.textRatio}%. Code bloat detected.`, 'Clean up HTML, reduce scripts/inline CSS.');\n  }\n  \n  // Programmatic: DOM Bloat\n  if (page.domNodeCount > 1500) {\n      addIssue('dom-bloat', 'Performance', IssueSeverity.MEDIUM, 'Excessive DOM Size', `Found ${page.domNodeCount} nodes.`, 'Reduce DOM complexity (< 1500 nodes) for rendering performance.');\n  }\n\n  // --- 5. HEADINGS ---\n  if (!page.h1) {\n    addIssue('missing-h1', 'Content', IssueSeverity.HIGH, 'Missing H1', 'No H1 tag.', 'Add exactly one H1.');\n  } else if (page.h1.length > 70) {\n    addIssue('long-h1', 'Content', IssueSeverity.LOW, 'H1 Too Long', 'H1 is very long.', 'Keep H1 concise.');\n  }\n  \n  if (page.h2s.length === 0 && page.h3s.length > 0) {\n    addIssue('skipped-heading', 'Structure', IssueSeverity.MEDIUM, 'Skipped Heading Level', 'Page has H3s but no H2s.', 'Maintain strict H1 -> H2 -> H3 hierarchy.');\n  }\n\n  // --- 6. INDEXATION & TECHNICAL ---\n  if (!page.canonical) {\n    addIssue('missing-canonical', 'Indexation', IssueSeverity.MEDIUM, 'Missing Canonical', 'No canonical tag.', 'Add self-referencing canonical.');\n  } else if (page.canonical !== page.url) {\n    const normUrl = page.url.endsWith('/') ? page.url.slice(0, -1) : page.url;\n    const normCanon = page.canonical.endsWith('/') ? page.canonical.slice(0, -1) : page.canonical;\n\n    // Check if the difference is just casing or slug\n    let urlPath = '';\n    let canonPath = '';\n    try {\n        urlPath = new URL(page.url).pathname;\n        canonPath = new URL(page.canonical).pathname;\n    } catch(e) {}\n\n    if (normUrl === normCanon) {\n        addIssue('canonical-slash', 'Indexation', IssueSeverity.HIGH, 'Trailing Slash Inconsistency', \n        `Canonical ${page.canonical} differs only by slash.`, 'Enforce trailing slash strategy.');\n    } else if (urlPath && canonPath && urlPath.toLowerCase() === canonPath.toLowerCase() && urlPath !== canonPath) {\n        addIssue('canonical-casing', 'Indexation', IssueSeverity.HIGH, 'URL Casing Issue', \n        `Accessed via \"${urlPath}\" but canonical is \"${canonPath}\".`, 'Ensure internal links use correct casing.');\n    } else {\n        addIssue('canonicalized', 'Indexation', IssueSeverity.INFO, 'Canonicalized', `Points to ${page.canonical}.`, 'Check if intentional.');\n    }\n  }\n\n  // Programmatic: Pagination\n  if (page.url.match(/page\\/\\d+/) || page.url.match(/p=\\d+/)) {\n      if (!page.relPrev && !page.relNext) {\n          addIssue('missing-pagination', 'Structure', IssueSeverity.MEDIUM, 'Pagination Tags Missing', 'Page looks paginated but lacks rel=\"next/prev\".', 'Add pagination tags.');\n      }\n  }\n\n  // International\n  if (page.hreflangs.length > 0) {\n      // check self reference\n      const hasSelf = page.hreflangs.some(h => h.url === page.url || h.url === page.url + '/' || h.url + '/' === page.url);\n      if (!hasSelf) {\n          addIssue('missing-self-hreflang', 'International', IssueSeverity.MEDIUM, 'Missing Self-Ref Hreflang', 'Hreflang tags found but no self-reference.', 'Add self-referencing hreflang tag.');\n      }\n  }\n\n  // --- 7. IMAGE SEO (VISUAL/STOCK SITES) ---\n  let missingAltCount = 0;\n  let missingDimsCount = 0;\n  let largeImgCount = 0;\n  let oldFormatCount = 0;\n  let missingTitleCount = 0;\n\n  page.images.forEach(img => {\n    // Skip tracking pixels\n    if (img.src.includes('pixel') || img.src.includes('analytics')) return;\n\n    if (!img.alt || img.alt.trim() === '') missingAltCount++;\n    else if (img.alt.length > 125) addIssue('long-alt', 'Images', IssueSeverity.LOW, 'Alt Text Too Long', 'Alt text > 125 chars.', 'Keep alt text concise.');\n\n    if (!img.width || !img.height) missingDimsCount++;\n    \n    // Heuristic: WebP/AVIF check\n    if (!img.src.match(/\\.(webp|avif|svg)$/i) && !img.src.startsWith('data:')) {\n         oldFormatCount++;\n    }\n\n    if (!img.title) missingTitleCount++;\n\n    // Protocol check\n    if (page.url.startsWith('https') && img.src.startsWith('http:')) {\n        addIssue('mixed-content-img', 'Security', IssueSeverity.HIGH, 'Insecure Image', `Image loaded over HTTP: ${img.src}`, 'Use HTTPS for assets.');\n    }\n  });\n\n  if (missingAltCount > 0) addIssue('missing-alt', 'Images', IssueSeverity.MEDIUM, 'Missing Alt Text', `${missingAltCount} images lack alt text.`, 'Add descriptive alt text.');\n  if (missingDimsCount > 0) addIssue('cls-risk', 'Images', IssueSeverity.HIGH, 'Missing Dimensions (CLS)', `${missingDimsCount} images lack width/height.`, 'Add width/height to prevent layout shifts.');\n  if (oldFormatCount > 2) addIssue('legacy-format', 'Images', IssueSeverity.LOW, 'Legacy Image Formats', `${oldFormatCount} images are not WebP/AVIF.`, 'Serve images in modern formats.');\n  if (missingTitleCount > 5) addIssue('missing-img-title', 'Images', IssueSeverity.INFO, 'Missing Image Titles', 'Many images lack title attributes.', 'Add title attributes for better UX.');\n\n  // --- 8. SCHEMA.ORG ---\n  if (page.schemas.length === 0) {\n      addIssue('missing-schema', 'Schema', IssueSeverity.LOW, 'No Structured Data', 'No JSON-LD found.', 'Add Schema (Article, Product, Breadcrumb) for rich snippets.');\n  } else {\n      page.schemas.forEach(s => {\n          if (!s.isValid) {\n              addIssue('invalid-schema', 'Schema', IssueSeverity.CRITICAL, 'Invalid JSON-LD', `Parse Error: ${s.error}`, 'Fix JSON syntax errors in schema.');\n          }\n      });\n  }\n\n  // --- 9. SECURITY & PERFORMANCE ---\n  if (!page.viewport) {\n    addIssue('no-viewport', 'Technical', IssueSeverity.CRITICAL, 'Missing Viewport', 'Mobile responsiveness issues.', 'Add viewport meta tag.');\n  }\n  \n  if (page.unsafeAnchorCount > 0) {\n      addIssue('unsafe-target', 'Security', IssueSeverity.MEDIUM, 'Unsafe Cross-Origin Links', `${page.unsafeAnchorCount} links use target=\"_blank\" without rel=\"noopener\".`, 'Add rel=\"noopener\" to external links.');\n  }\n\n  if (page.inlineCssCount > 20) {\n      addIssue('inline-css', 'Performance', IssueSeverity.LOW, 'Excessive Inline CSS', `${page.inlineCssCount} elements with style attribute.`, 'Move styles to external CSS files.');\n  }\n\n  if (page.loadTime > 2000) {\n    addIssue('slow-response', 'Performance', IssueSeverity.MEDIUM, 'Slow Response', `TTFB ${page.loadTime}ms.`, 'Optimize server.');\n  }\n\n  return issues;\n};\n"
            }
          ]
        }
      ]
    },
    "createdTimestamp": {
      "seconds": 1770901205,
      "nanos": 403000000
    },
    "doneTimestamp": {
      "seconds": 1770901217,
      "nanos": 229000000
    }
  },
  {
    "id": 6,
    "author": "user",
    "payload": {
      "type": "text",
      "text": "this is counting image urls as pages, wtf",
      "files": [],
      "fileUploads": [],
      "cssBlocks": [],
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770934966,
      "nanos": 165000000
    }
  },
  {
    "id": 7,
    "author": "model",
    "payload": {
      "type": "thinking",
      "text": "**Investigating Image URL Inclusion**\n\nI'm currently focused on the issue of image URLs appearing as pages in the crawl results. I'm digging into the `Crawler` class within `services/crawler.ts` to understand how links are extracted and added to the queue, hoping to locate the root cause. I will also be reviewing `types.ts` for related structures.\n\n\n**Analyzing Link Extraction Logic**\n\nI've been examining the `crawlPage` method within `services/crawler.ts`, specifically the link extraction section. I see how the crawler identifies `a[href]` elements and processes their URLs. The code handles `javascript:`, `mailto:`, and `tel:` prefixes. The `anchorText` is being parsed. I'm noting the handling of absolute URLs via the `URL` constructor. The hash removal is also crucial. I'm focusing on why image URLs are being inappropriately added.\n\n\n**Implementing URL Filtering**\n\nI've pinpointed the problem: the current implementation treats image URLs as valid pages. My solution is to introduce a filtering mechanism within `services/crawler.ts`. Specifically, I'll modify the `linkElements.forEach` loop to check the `cleanUrl` for common media file extensions before adding the URL to the crawl queue, thus excluding image URLs. I'm compiling a comprehensive list of extensions to exclude, including `.jpg`, `.png`, `.pdf`, and more, to ensure a thorough filter.\n\n\n**Refining Filtering Strategy**\n\nI'm refining my filtering strategy based on new insights. While I initially focused on preventing image URLs from entering the crawl queue, I realize they're valuable as internal links for SEO audits. Therefore, I'll revise my approach to add these URLs to `internalLinks` to track references, but they'll be excluded from the `queue` to avoid unnecessary crawling and their inclusion as pages. This should eliminate the problem of images appearing as distinct \"pages\" in the results. The goal is to avoid unnecessary calls to `crawlPage`.\n\n\n**Implementing Exclusion List**\n\nI've crafted a comprehensive exclusion list for the image URL filter, now including not just images but also documents, media, code, styles, and fonts, to be added to `services/crawler.ts`. The implementation uses a helper function, now called `isResourceUrl`, that checks if a URL's pathname ends with any of the defined extensions. I'll integrate this check within the URL processing loop to keep image URLs from unnecessarily entering `this.queue`, as preventing that is the ultimate fix.\n\n\n**Refining Resource URL Check**\n\nI've taken the existing `isResourceUrl` helper function and am integrating it more deeply into the `crawlPage` method within `services/crawler.ts`. Specifically, I am modifying the `linkElements.forEach` loop to use `isResourceUrl` to filter URLs. This should prevent unnecessary calls to `crawlPage` for non-page resources, while still keeping these URLs tracked as internal links, which I'll add to a dedicated list. This implementation keeps both efficiency and internal link tracking in mind.\n\n\n**Confirming Image Link Behavior**\n\nI've just confirmed the behavior of image URLs as internal links. I'm satisfied that including them in `internalLinks`, defined as `{ url, text }` in `types.ts`, and subsequently displayed in the \"Links\" section on the source page, is the desired outcome. This aligns with standard crawler practices, listing image links as internal connections, without unnecessary crawling.\n\n\n**Verifying Content Type Handling**\n\nI'm now incorporating content type handling in `services/crawler.ts` to address edge cases like URLs without extensions, such as `/images/logo`. I'll use the `fetch` response headers to check the `content-type`. If it's not HTML, I'll prevent the URL from generating a `PageData` entry, thus stopping it from appearing as a \"Page\" in the main dashboard. This should solve the original problem. I'll verify if `types.ts`, `App.tsx`, `services/postAnalysis.ts`, `services/auditor.ts`, `components/Dashboard.tsx`, and `components/IssueDetails.tsx` need changes, and update the crawler accordingly.\n\n\n**Refining Content Type Handling**\n\nI'm revisiting content type detection within `crawlPage`, focusing on edge cases of URLs like `/images/logo` lacking extensions. If `isResourceUrl` fails but the crawler proceeds to fetch the URL, I'll now check the `Content-Type` header within `services/crawler.ts`. If it's not `text/html` or `application/xhtml+xml`, I'll prevent `this.onPageCrawled(pageData)` from being called, effectively excluding the URL from the \"Pages\" report. I'm verifying all related components and services to determine if any adjustments are necessary.\n\n\n**Improving Content Type Handling**\n\nI'm now implementing the refined content-type logic within the `crawlPage` method to accurately classify URLs. I will directly incorporate a check for the `Content-Type` header after the fetch operation, preventing non-HTML resources from being processed as pages. If the `Content-Type` is not HTML, I'll bypass the `onPageCrawled` call, thus keeping these resource URLs out of the \"Pages\" report. I am also checking related components and services for adjustments.\n\n\n**Implementing Content Type Check**\n\nI'm now incorporating a `Content-Type` check within the fetch response within `services/crawler.ts`. If the content type isn't HTML, the URL is handled as a non-page resource. I will consume the response body, so the fetch is complete, then bypass calling `onPageCrawled`. This refined logic should address the issue where URLs return responses with non-HTML content, but are incorrectly counted as \"pages.\"\n\n\n**Refining Exclusion Implementation**\n\nI'm now integrating content type checks with the existing resource URL filtering for added robustness. Within `services/crawler.ts`, I'm ensuring both the URL extensions and content-type are verified to avoid adding non-page resources. Specifically, I'm using `isResourceUrl` to filter extensions first. Then, if the extension check fails, I will fetch the resource and inspect the `Content-Type` header. If the type isn't HTML, the URL is not added to the pages list, but will be tracked.\n\n\n**Integrating Resource Filtering**\n\nI'm now integrating the `isResourceUrl` function at the top of `services/crawler.ts`. I've also integrated the function into the link extraction loop, to prevent unnecessary fetches and keep the queue efficient. This is in conjunction with content type handling to ensure all non-HTML resources are properly excluded from the \"Pages\" report. I'm verifying that the crawl process correctly handles these non-page resources and ensures efficient crawling.\n\n\n\n**Addressing Start URL Behavior**\n\nI'm now focusing on how the crawler handles image URLs entered as the `startUrl`. I'm verifying that even if an image URL is initially provided as the start point, the refined filtering logic within `crawlPage`, which includes content-type checks and `isResourceUrl`, prevents these resources from being incorrectly processed as \"pages\" in the crawl. The goal is to address the issue, so image URLs entered as a `startUrl` don't lead to incorrect states.\n\n\n**Refining Crawl Completion**\n\nI'm now clarifying the behavior when an image URL, or other non-HTML resource, is the starting URL. My intention is to ensure the tool correctly reports that the crawl is complete, even if the starting URL is not an HTML page. While the process currently concludes when the queue is empty, and I understand the current completion behavior is acceptable, I'll revise my strategy to also report these resources separately, which can be done by tracking the image URLs in the `internalLinks` structure. I am also planning an update to track non-HTML resources.\n\n\n\n**Clarifying Crawl Completion**\n\nI'm now clarifying the logic for reporting crawl completion with non-HTML start URLs. I'll ensure the tool reports completion accurately, even when starting with an image or other non-HTML resources. My primary focus is to ensure that the crawl concludes and the state is properly reflected, with non-HTML resources tracked separately. I'm updating `services/crawler.ts`.\n\n```typescript\n<change>\n<file>services/crawler.ts</file>\n<description>Ensure crawl completion is correctly reported, even with non-HTML start URLs, and revise pageData to track both links and resources.</description>\n<content><![CDATA[\nimport { PageData, CrawlSettings, ImageAsset, SchemaData, InternalLink } from '../types';\nimport { analyzePage } from './auditor';\n// Helper to normalize URLs\nconst normalizeUrl = (url: string): string => {\n  let u = url.trim();\n  // Auto-prepend protocol if missing\n  if (!u.match(/^https?:\\/\\//i)) {\n    u = 'https://' + u;\n  }\n  try {\n    const urlObj = new URL(u);\n    urlObj.hash = ''; // Remove fragments\n    return urlObj.toString();\n  } catch (e) {\n    return '';\n  }\n};\n// Simple DJB2 hash for content fingerprinting\nconst simpleHash = (str: string): string => {\n  let hash = 5381;\n  for (let i = 0; i < str.length; i++) {\n    hash = (hash * 33) ^ str.charCodeAt(i);\n  }\n  return hash.toString(36);\n};\n// Function to determine if a URL is a resource\nconst isResourceUrl = (url: string): boolean => {\n  const resourceExtensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.svg', '.webp', '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.js', '.css', '.woff', '.woff2', '.ttf', '.eot', '.ico', '.mp3', '.mp4', '.avi', '.mov', '.zip', '.rar', '.exe'];\n  return resourceExtensions.some(ext => url.toLowerCase().endsWith(ext));\n};\nexport class Crawler {\n  private readonly startUrl: string;\n  private readonly settings: CrawlSettings;\n  private queue: string[] = [];\n  private visited: Set<string> = new Set();\n  private internalLinks: InternalLink[] = [];\n  private onPageCrawled: (data: PageData) => void;\n  private onCrawlComplete: (internalLinks: InternalLink[]) => void;\n  constructor(startUrl: string, settings: CrawlSettings, onPageCrawled: (data: PageData) => void, onCrawlComplete: (internalLinks: InternalLink[]) => void) {\n    this.startUrl = normalizeUrl(startUrl);\n    this.settings = settings;\n    this.onPageCrawled = onPageCrawled;\n    this.onCrawlComplete = onCrawlComplete;\n    this.queue.push(this.startUrl);\n  }\n  public async crawl(): Promise<void> {\n    while (this.queue.length > 0) {\n      const url = this.queue.shift()!;\n      if (this.visited.has(url)) {\n        continue;\n      }\n      this.visited.add(url);\n      try {\n        await this.crawlPage(url);\n      } catch (error) {\n        console.error(`Failed to crawl ${url}:`, error);\n        // Consider how to handle individual failures\n      }\n    }\n    // Call the crawl complete callback after the queue is empty\n    this.onCrawlComplete(this.internalLinks);\n  }\n  private async crawlPage(url: string): Promise<void> {\n    try {\n      const response = await fetch(url);\n      if (!response.ok) {\n        console.error(`HTTP error fetching ${url}: ${response.status}`);\n        return;\n      }\n      const contentType = response.headers.get('content-type') || '';\n      // If content type is not HTML, skip processing this as a page\n      if (!contentType.includes('text/html') && !contentType.includes('application/xhtml+xml')) {\n        // We still consume the response body to avoid hanging the connection.\n        await response.text();\n        return;\n      }\n\n\n**Finalizing Non-HTML Start URLs**\n\nI'm now completing my work on the crawler. I've ensured non-HTML start URLs are correctly handled. The crawler will still run and the `onCrawlComplete` callback is called, even if the start URL isn't an HTML page. These URLs are now tracked separately and the crawl completes cleanly, as intended.\n\n\n**Refining Crawl Completion**\n\nI'm verifying the completion logic. Now, the crawler finishes accurately. Even with a non-HTML start URL, I make sure the crawl properly ends. I update the `Crawler` class, ensuring the `onCrawlComplete` callback triggers, and non-HTML resources are distinctly tracked as \"internalLinks.\"\n\n\n**Refining Crawl Logic**\n\nI'm now integrating proxy fallback logic within `services/crawler.ts`, which includes multiple proxies with deduplication, in the crawler to enhance accessibility. The crawl now efficiently handles potential connection errors, incorporating a prioritized proxy strategy.\n\n\n**Implementing Proxy Fallback**\n\nI'm now integrating robust proxy handling to improve crawl reliability, specifically in `services/crawler.ts`. The proxy strategy now includes multiple proxies with deduplication, handling potential connection errors efficiently with a retry mechanism and incorporating a 15-second timeout, which improves accessibility.\n\n\n**Verifying Crawler Completion**\n\nThe crawler now finishes correctly, even if I start with a non-HTML URL, such as an image. I've confirmed that the `onCrawlComplete` callback is called, ensuring the crawl status is reported. The logic within `services/crawler.ts` distinctly tracks non-HTML resources.\n\n\n**Confirming Final Implementation**\n\nI've just finalized the implementation. I've ensured a robust and reliable crawling process. I've verified that even non-HTML start URLs now function correctly and trigger the `onCrawlComplete` callback. This includes proxy fallback and retries, which is crucial for overall reliability.\n\n\n**Validating Final Crawl Logic**\n\nI'm making final assessments of the updated crawler. I'm satisfied that non-HTML start URLs are correctly processed, triggering completion. I'm ensuring robust proxy handling is fully integrated for reliability. The updated crawler ensures reliable and comprehensive crawls, and the tool correctly reports and terminates after any valid starting resource.\n\n\n**Refining Data Structures**\n\nI'm now integrating header data extraction, namely `h1`, `h2`, and `h3` elements, with canonical and robots meta tags within the `crawlPage` method in `services/crawler.ts`. I am also adding pagination and hreflang metadata extraction, using relevant query selectors and data structures.\n\n\n**Enhancing Data Extraction**\n\nI'm now implementing meta data extraction. I'm focusing on canonical URLs, social tags, and schema.org data within `services/crawler.ts`. I am adding new query selectors to parse various metadata elements. The goal is enriching page analysis for the user.\n\n\n**Refining Internal Link Tracking**\n\nI'm now integrating internal link details, including the associated text, within the `Crawler` class in `services/crawler.ts`. I am refining my link handling to track each internal link's associated text and ensure accurate data is stored.\n\n\n**Refining Meta Data Extraction**\n\nI'm now integrating the extraction of various metadata elements within `services/crawler.ts`. I am adding canonical and robots meta tag parsing, to improve the depth and accuracy of page analysis. I am also checking for pagination, and will be adding hreflang metadata extraction, to better support content auditing.\n\n\n**Finalizing Resource Exclusion**\n\nMy current focus is ensuring robust exclusion for resources. I've been verifying that the content type check within `crawlPage` of `services/crawler.ts` now prevents the creation of page data entries for non-HTML resources. This makes those resources be effectively excluded and correctly tracked as internal links.\n\n\n**Verifying Image Exclusion**\n\nI'm now verifying that non-page resources are consistently tracked as `internalLinks`. My update includes the processing of image, script, and CSS URLs, to make sure they are not improperly categorized as pages. The goal is accurate crawling.\n\n\n**Refining Failed Request Handling**\n\nI'm now implementing more robust error handling within `services/crawler.ts`. I will modify the error handling within the crawler to populate a default `PageData` object for failed requests. This provides a consistent data structure, even when a fetch fails.\n\n\n",
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770934966,
      "nanos": 165000000
    },
    "doneTimestamp": {
      "seconds": 1770935041,
      "nanos": 634000000
    }
  },
  {
    "id": 8,
    "author": "model",
    "payload": {
      "type": "text",
      "text": "I have updated `services/crawler.ts` to strictly filter out non-HTML resources.\n\n**Key Changes:**\n1.  **Extension Filtering**: Added an `isResourceUrl` helper that checks the URL path against a comprehensive list of non-HTML extensions (images, docs, media, fonts, code). This prevents adding these URLs to the crawl queue during link extraction.\n2.  **Content-Type Validation**: Added a check inside `crawlPage` to inspect the `Content-Type` header of the fetched response. If the response is `200 OK` but the content type is not `text/html` (or `application/xhtml+xml`), the crawler consumes the stream (to be polite) but **immediately returns**, preventing `onPageCrawled` from firing. This ensures no image or asset rows ever appear in your dashboard \"Pages\" list.\n\n",
      "cMarkNode": {}
    },
    "createdTimestamp": {
      "seconds": 1770935041,
      "nanos": 634000000
    },
    "doneTimestamp": {
      "seconds": 1770935041,
      "nanos": 813000000
    }
  },
  {
    "id": 9,
    "author": "model",
    "payload": {
      "type": "generationTable",
      "entries": [
        {
          "path": "services/crawler.ts",
          "generationType": 1,
          "isDone": true,
          "description": "Filter out non-HTML resources (images, PDFs, JS, CSS) from the crawl queue and verify Content-Type to prevent assets from appearing as pages.",
          "diffs": [
            {
              "target": "",
              "replacement": "\nimport { PageData, CrawlSettings, ImageAsset, SchemaData, InternalLink } from '../types';\nimport { analyzePage } from './auditor';\n\n// Helper to normalize URLs\nconst normalizeUrl = (url: string): string => {\n  let u = url.trim();\n  // Auto-prepend protocol if missing\n  if (!u.match(/^https?:\\/\\//i)) {\n    u = 'https://' + u;\n  }\n  try {\n    const urlObj = new URL(u);\n    urlObj.hash = ''; // Remove fragments\n    return urlObj.toString();\n  } catch (e) {\n    return '';\n  }\n};\n\n// Simple DJB2 hash for content fingerprinting\nconst simpleHash = (str: string): string => {\n  let hash = 5381;\n  for (let i = 0; i < str.length; i++) {\n    hash = (hash * 33) ^ str.charCodeAt(i);\n  }\n  return (hash >>> 0).toString(16);\n};\n\n// Helper to check if URL is a resource/file that shouldn't be crawled as a page\nconst isResourceUrl = (url: string): boolean => {\n  try {\n    const pathname = new URL(url).pathname.toLowerCase();\n    const extensions = [\n      // Images\n      '.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.ico', '.bmp', '.tiff', '.avif',\n      // Documents\n      '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.txt', '.csv', '.zip', '.rar', '.7z', '.gz',\n      // Media\n      '.mp4', '.mp3', '.avi', '.mov', '.wav', '.webm', '.ogg',\n      // Code/Styles/Data\n      '.css', '.js', '.json', '.xml', '.rss', '.atom', '.map',\n      // Fonts\n      '.woff', '.woff2', '.ttf', '.eot', '.otf'\n    ];\n    return extensions.some(ext => pathname.endsWith(ext));\n  } catch(e) { return false; }\n};\n\ninterface QueueItem {\n  url: string;\n  depth: number;\n}\n\nexport class Crawler {\n  private visited = new Set<string>();\n  private queue: QueueItem[] = [];\n  private settings: CrawlSettings;\n  private isRunning = false;\n  private onPageCrawled: (page: PageData) => void;\n  private onComplete: () => void;\n\n  constructor(\n    settings: CrawlSettings, \n    onPageCrawled: (page: PageData) => void,\n    onComplete: () => void\n  ) {\n    this.settings = settings;\n    this.onPageCrawled = onPageCrawled;\n    this.onComplete = onComplete;\n  }\n\n  start(startUrl: string) {\n    if (this.isRunning) return;\n    this.isRunning = true;\n    this.visited.clear();\n    const normalized = normalizeUrl(startUrl);\n    if (!normalized) {\n        alert(\"Invalid Start URL\");\n        this.stop();\n        return;\n    }\n    this.queue = [{ url: normalized, depth: 0 }];\n    this.processQueue();\n  }\n\n  stop() {\n    this.isRunning = false;\n    this.onComplete();\n  }\n\n  private async processQueue() {\n    if (!this.isRunning) return;\n    if (this.queue.length === 0 || this.visited.size >= this.settings.maxPages) {\n      this.stop();\n      return;\n    }\n\n    const item = this.queue.shift();\n    if (!item) return;\n\n    if (this.visited.has(item.url)) {\n      this.processQueue();\n      return;\n    }\n\n    if (item.depth > this.settings.maxDepth) {\n        this.processQueue(); // Skip but keep processing\n        return;\n    }\n\n    this.visited.add(item.url);\n    await this.crawlPage(item);\n\n    // Randomize delay slightly to appear more human\n    const delay = this.settings.crawlSpeed + (Math.random() * 200);\n    setTimeout(() => {\n      this.processQueue();\n    }, delay);\n  }\n\n  private async crawlPage(item: QueueItem) {\n    const startTime = performance.now();\n    \n    // Define Proxy Fallback Strategy\n    const proxies = [\n        this.settings.proxyUrl,                  // User configured (Default: corsproxy.io)\n        'https://api.allorigins.win/raw?url=',   // Fallback 1\n        'https://thingproxy.freeboard.io/fetch/' // Fallback 2\n    ].filter(p => !!p);\n    \n    // Deduplicate proxies\n    const uniqueProxies = [...new Set(proxies)];\n\n    let rawHtml = '';\n    let status = 0;\n    let fetchSuccess = false;\n\n    // Try proxies in order\n    for (const proxyBase of uniqueProxies) {\n        if (fetchSuccess) break;\n\n        // Construct Proxy URL (Always encodeURIComponent for safety with these proxies)\n        // corsproxy.io supports ?url= or just ? but encoding is safer for params\n        const proxyUrl = `${proxyBase}${encodeURIComponent(item.url)}`;\n        \n        const MAX_RETRIES = 2; // Retries per proxy\n        \n        for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {\n            try {\n                const controller = new AbortController();\n                const timeoutId = setTimeout(() => controller.abort(), 15000); // 15s timeout\n\n                const response = await fetch(proxyUrl, {\n                    signal: controller.signal,\n                    headers: {\n                        'Cache-Control': 'no-cache',\n                        // Some proxies require X-Requested-With to allow access\n                        'X-Requested-With': 'XMLHttpRequest' \n                    }\n                });\n                clearTimeout(timeoutId);\n\n                status = response.status;\n                \n                // If we get a 200-500 response, the network request worked.\n                // If we get a 0 or throw, it's a network error.\n                \n                // Note: Some proxies return 200 even if they fail internally, returning JSON error.\n                // We will check that later in parsing.\n                \n                if (status === 429) {\n                    throw new Error('Rate Limited');\n                }\n                \n                // Content Type Check to avoid crawling images/PDFs as pages\n                const contentType = response.headers.get('content-type') || '';\n                // If it's 200 OK but not HTML, skip processing as a page\n                if (status === 200 && !contentType.includes('text/html') && !contentType.includes('application/xhtml+xml')) {\n                     // Consume body to finish request but return early\n                     await response.text();\n                     return; \n                }\n\n                rawHtml = await response.text();\n                \n                // Check if the proxy returned an error message in HTML disguise\n                if (rawHtml.includes('Could not fetch URL') || rawHtml.includes('Access to the requested resource is forbidden')) {\n                    throw new Error('Proxy failed to fetch target');\n                }\n\n                fetchSuccess = true;\n                break; // Break retry loop\n            } catch (error: any) {\n                console.warn(`Proxy ${proxyBase} failed for ${item.url} (Attempt ${attempt}): ${error.message}`);\n                \n                if (attempt < MAX_RETRIES) {\n                     await new Promise(r => setTimeout(r, 1000));\n                }\n            }\n        }\n    }\n\n    if (!fetchSuccess) {\n        console.error(`All proxies failed for ${item.url}`);\n        // Log a synthetic \"Network Error\" page so the user knows it failed\n        const failData: PageData = this.createFailedPageData(item.url, 0);\n        this.onPageCrawled(failData);\n        return;\n    }\n\n    const loadTime = Math.round(performance.now() - startTime);\n    const size = new Blob([rawHtml]).size;\n\n    const parser = new DOMParser();\n    const doc = parser.parseFromString(rawHtml, 'text/html');\n\n    // --- Deep Extraction ---\n\n    // 1. Content & Structure\n    const bodyText = doc.body ? doc.body.innerText : '';\n    const cleanText = bodyText.replace(/\\s+/g, ' ').trim();\n    const wordCount = cleanText.length > 0 ? cleanText.split(' ').length : 0;\n    const textRatio = size > 0 ? Math.round((cleanText.length / rawHtml.length) * 100) : 0;\n    const contentHash = simpleHash(cleanText);\n    const domNodeCount = doc.getElementsByTagName('*').length;\n\n    // 2. Meta Headers\n    const title = doc.querySelector('title')?.innerText.trim() || null;\n    const description = doc.querySelector('meta[name=\"description\"]')?.getAttribute('content')?.trim() || null;\n    const viewport = doc.querySelector('meta[name=\"viewport\"]')?.getAttribute('content') || null;\n    const charset = doc.characterSet || doc.inputEncoding;\n\n    // 3. Header Structure\n    const h1 = doc.querySelector('h1')?.innerText.trim() || null;\n    const h2s = Array.from(doc.querySelectorAll('h2')).map(el => el.innerText.trim()).filter(Boolean);\n    const h3s = Array.from(doc.querySelectorAll('h3')).map(el => el.innerText.trim()).filter(Boolean);\n\n    // 4. Canonicals, Robots, Pagination, Hreflang\n    const canonicalLink = doc.querySelector('link[rel=\"canonical\"]')?.getAttribute('href') || null;\n    const metaRobots = doc.querySelector('meta[name=\"robots\"]')?.getAttribute('content') || null;\n    const relNext = doc.querySelector('link[rel=\"next\"]')?.getAttribute('href') || null;\n    const relPrev = doc.querySelector('link[rel=\"prev\"]')?.getAttribute('href') || null;\n\n    const hreflangs = Array.from(doc.querySelectorAll('link[rel=\"alternate\"][hreflang]')).map(el => ({\n        lang: el.getAttribute('hreflang') || '',\n        url: el.getAttribute('href') || ''\n    }));\n\n    // Resolve Canonical\n    let absoluteCanonical = null;\n    if (canonicalLink) {\n      try {\n        absoluteCanonical = new URL(canonicalLink, item.url).toString();\n      } catch (e) {\n        absoluteCanonical = canonicalLink; \n      }\n    }\n\n    // 5. Social Tags\n    const ogTitle = doc.querySelector('meta[property=\"og:title\"]')?.getAttribute('content') || null;\n    const ogImage = doc.querySelector('meta[property=\"og:image\"]')?.getAttribute('content') || null;\n    const twitterCard = doc.querySelector('meta[name=\"twitter:card\"]')?.getAttribute('content') || null;\n\n    // 6. Schema.org Extraction\n    const schemas: SchemaData[] = [];\n    const scriptTags = Array.from(doc.querySelectorAll('script[type=\"application/ld+json\"]'));\n    scriptTags.forEach(script => {\n        const content = script.textContent || '';\n        try {\n            const json = JSON.parse(content);\n            schemas.push({\n                type: json['@type'] || 'Unknown',\n                raw: JSON.stringify(json, null, 2),\n                isValid: true\n            });\n        } catch (e) {\n            schemas.push({\n                type: 'Invalid JSON',\n                raw: content,\n                isValid: false,\n                error: (e as Error).message\n            });\n        }\n    });\n\n    // 7. Assets & Images (Deep)\n    const images: ImageAsset[] = Array.from(doc.querySelectorAll('img')).map(img => {\n        const src = img.getAttribute('src') || '';\n        let isExternal = false;\n        try { isExternal = new URL(src, item.url).hostname !== new URL(item.url).hostname; } catch(e){}\n        \n        return {\n            src,\n            alt: img.getAttribute('alt') || '',\n            title: img.getAttribute('title') || null,\n            width: img.getAttribute('width') || null,\n            height: img.getAttribute('height') || null,\n            loading: img.getAttribute('loading') || null,\n            isExternal\n        };\n    });\n    \n    const scripts = Array.from(doc.querySelectorAll('script'));\n    const cssLinks = Array.from(doc.querySelectorAll('link[rel=\"stylesheet\"]'));\n    const inlineCssCount = Array.from(doc.querySelectorAll('[style]')).length;\n\n    // 8. Security & Tech\n    let analyticsId = null;\n    if (rawHtml.match(/UA-\\d+-\\d+/)) analyticsId = rawHtml.match(/UA-\\d+-\\d+/)?.[0] || null;\n    else if (rawHtml.match(/G-[A-Z0-9]+/)) analyticsId = rawHtml.match(/G-[A-Z0-9]+/)?.[0] || null;\n\n    const deprecatedTags = [];\n    if (doc.querySelector('center')) deprecatedTags.push('<center>');\n    if (doc.querySelector('font')) deprecatedTags.push('<font>');\n    if (doc.querySelector('marquee')) deprecatedTags.push('<marquee>');\n\n    const emailsFound = (bodyText.match(/([a-zA-Z0-9._-]+@[a-zA-Z0-9._-]+\\.[a-zA-Z0-9._-]+)/gi) || [])\n        .filter((v, i, a) => a.indexOf(v) === i); \n\n    const unsafeAnchorCount = Array.from(doc.querySelectorAll('a[target=\"_blank\"]:not([rel*=\"noopener\"])')).length;\n\n    // 9. Link Extraction\n    const internalLinks: InternalLink[] = [];\n    const externalLinks: string[] = [];\n    \n    let baseDomain = '';\n    try {\n      baseDomain = new URL(item.url).hostname;\n    } catch(e) {}\n\n    const linkElements = Array.from(doc.querySelectorAll('a[href]'));\n    linkElements.forEach(el => {\n      const href = el.getAttribute('href');\n      if (!href) return;\n      if (href.startsWith('javascript:') || href.startsWith('mailto:') || href.startsWith('tel:')) return;\n\n      const anchorText = (el as HTMLElement).innerText?.replace(/\\s+/g, ' ').trim().slice(0, 50) || 'Empty/Image';\n\n      try {\n        const absoluteUrl = new URL(href, item.url).toString();\n        const urlObj = new URL(absoluteUrl);\n        \n        // Remove hash from crawled links\n        urlObj.hash = '';\n        const cleanUrl = urlObj.toString();\n        \n        // Pre-check for resource extensions to avoid queuing them\n        const isResource = isResourceUrl(cleanUrl);\n        \n        if (urlObj.hostname === baseDomain) {\n            internalLinks.push({ url: cleanUrl, text: anchorText });\n            // Only add to queue if NOT a resource\n            if (!this.visited.has(cleanUrl) && !isResource) {\n                 this.queue.push({ url: cleanUrl, depth: item.depth + 1 });\n            }\n        } else {\n            externalLinks.push(cleanUrl);\n        }\n      } catch (e) {\n        // Invalid URL\n      }\n    });\n\n    const pageData: PageData = {\n      url: item.url,\n      status,\n      loadTime,\n      size,\n      wordCount,\n      textRatio,\n      contentHash,\n      domNodeCount,\n      title,\n      description,\n      h1,\n      h2s,\n      h3s,\n      canonical: absoluteCanonical,\n      metaRobots,\n      viewport,\n      charset: charset || 'unknown',\n      relNext,\n      relPrev,\n      hreflangs,\n      ogTitle,\n      ogImage,\n      twitterCard,\n      analyticsId,\n      deprecatedTags,\n      emailsFound,\n      schemas,\n      images,\n      scriptCount: scripts.length,\n      cssCount: cssLinks.length,\n      inlineCssCount,\n      internalLinks,\n      externalLinks,\n      unsafeAnchorCount,\n      inRank: 0, \n      inlinksCount: 0, \n      issues: [],\n      isIndexable: status === 200 && !metaRobots?.includes('noindex'),\n      depth: item.depth,\n    };\n\n    pageData.issues = analyzePage(pageData);\n\n    this.onPageCrawled(pageData);\n  }\n\n  // Helper to create a page data object for failed requests\n  private createFailedPageData(url: string, status: number): PageData {\n      const p: PageData = {\n          url,\n          status,\n          loadTime: 0,\n          size: 0,\n          wordCount: 0,\n          textRatio: 0,\n          contentHash: '',\n          domNodeCount: 0,\n          title: null,\n          description: null,\n          h1: null,\n          h2s: [],\n          h3s: [],\n          canonical: null,\n          metaRobots: null,\n          viewport: null,\n          charset: null,\n          relNext: null,\n          relPrev: null,\n          hreflangs: [],\n          ogTitle: null,\n          ogImage: null,\n          twitterCard: null,\n          analyticsId: null,\n          deprecatedTags: [],\n          emailsFound: [],\n          schemas: [],\n          images: [],\n          scriptCount: 0,\n          cssCount: 0,\n          inlineCssCount: 0,\n          internalLinks: [],\n          externalLinks: [],\n          unsafeAnchorCount: 0,\n          inRank: 0,\n          inlinksCount: 0,\n          issues: [],\n          isIndexable: false,\n          depth: 0\n      };\n      p.issues = analyzePage(p);\n      return p;\n  }\n}\n"
            }
          ]
        }
      ]
    },
    "createdTimestamp": {
      "seconds": 1770935043,
      "nanos": 445000000
    },
    "doneTimestamp": {
      "seconds": 1770935076,
      "nanos": 267000000
    }
  }
]